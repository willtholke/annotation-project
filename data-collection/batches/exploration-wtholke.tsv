0	wtholke	disagree	"def benchmark(f: Callable[[], Any], iters: Optional[int] = None,
              warmup: Optional[int] = None, name: Optional[str] = None,
              target_total_secs: Optional[Union[int, float]] = None):
  """"""Benchmarks ``f``. Prints the results and returns the raw times.
  """""""
1	wtholke	disagree	"def benchmark_suite(prepare: Callable[..., Callable], params_list: List[Dict],
                    name: str, target_total_secs: Optional[int] = None):
  """"""Benchmarks a function for several combinations of parameters.
  """""""
2	wtholke	4	"def _get_baseline_means(baseline_dir, name):
  baseline_dir = os.path.expanduser(baseline_dir)
  filename = os.path.join(baseline_dir, name + "".csv"")
  if not os.path.exists(filename):
    raise FileNotFoundError(""Can't find baseline file: %s"" % filename)
  with open(filename, newline="""") as csvfile:
    reader = csv.reader(csvfile)
    header = next(reader)
    mean_idx = header.index(""mean"")
    return [float(row[mean_idx]) for row in reader]"
3	wtholke	5	"def _export_results(data_header, data, export_dir, name):
  assert ""mean"" in data_header # For future comparisons via _get_baseline_means
  export_dir = os.path.expanduser(export_dir)
  os.makedirs(export_dir, exist_ok=True)
  filename = os.path.join(export_dir, name + "".csv"")
  with open(filename, ""w"", newline="""") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(data_header)
    writer.writerows(data)
  return filename"
4	wtholke	3	"def _param_str(param):
  if callable(param):
    return param.__name__
  return str(param)"
5	wtholke	3	"def math_benchmark(*args):
  def decorator(func):
    for test_case in args[0]:"
6	wtholke	4	"def wrapper(state, test_case=test_case):
  return func(state, **test_case)"
7	wtholke	disagree	"def jax_unary(state, **kwargs):
  shape = kwargs['shape']
  dtype = kwargs['dtype']
  op = kwargs['op']
  input0 = np.random.random(shape).astype(dtype)
  f = op
  f_jitted = jax.jit(f)
  f_jitted(input0).block_until_ready()
  while state:
    f_jitted(input0).block_until_ready()
  state.counters['items_per_second'] = Counter(
      input0.size * state.iterations, Counter.kIsRate
  )"
8	wtholke	disagree	"def jax_binary_op(state, **kwargs):
  mkn = kwargs['mkn']
  m = mkn[0]
  k = mkn[1]
  n = mkn[2]
  dtype = kwargs['dtype']
  op = kwargs['op']
  a = np.random.random([m, k]).astype(dtype)
  b = np.random.random([k, n]).astype(dtype)
  f = op
  f_jitted = jax.jit(f)
  f_jitted(a, b).block_until_ready()
  while state:
    f_jitted(a, b).block_until_ready()
  state.counters['items_per_second'] = Counter(
      state.iterations, Counter.kIsRate
  )"
9	wtholke	disagree	"def get_regions():
    return requests.post(_API_URL, json={'name':'list_regions'},
                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['regions']"
10	wtholke	disagree	"def find_existing_cluster():
    return requests.post(_API_URL, json={'name':'find_cluster'},
                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['region']"
11	wtholke	disagree	"def get_cluster_ip(region):
    return requests.post(_API_URL, json={'name':'get_cluster_ip', 'region':region},
                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['cluster_ip']"
12	wtholke	disagree	"def get_cluster_username(region):
    return requests.post(_API_URL, json={'name':'get_cluster_username', 'region':region},
                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['cluster_username']"
13	wtholke	disagree	"def create_cluster(region):
    logging.debug(requests.post(_API_URL, json={'name':'create_cluster', 'region':region},
                                auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS))"
14	wtholke	4	"class DefaultReport:
  outcome : str = ""none"""
15	wtholke	disagree	"def parse_line(line):
  # TODO(jakevdp): should we parse other report types?
  parsed = json.loads(line)
  if parsed.get(""$report_type"") == ""TestReport"":
    return TestReport._from_json(parsed)
  return DefaultReport()"
16	wtholke	4	"def main(logfile, outfile):
  logging.info(""Parsing %s"", logfile)
  try:
    with open(logfile, 'r') as f:
      reports = (parse_line(line) for line in f)
      failures = (r for r in reports if r.outcome == ""failed"")
      summary = ""\n"".join(f""{f.nodeid}: {f.longrepr.chain[0][1].message}""
                          for f in failures)
    logging.info(""Parsed summary:\n%s"", summary)
  except Exception:
    err_info = traceback.format_exc()
    logging.info(""Parsing failed:\n%s"", err_info)
    summary = f""Log parsing failed; traceback:\n\n{err_info}""
  logging.info(""Writing result to %s"", outfile)
  with open(outfile, 'w') as f:
    f.write(MSG_FORMAT.format(summary=summary))"
17	wtholke	disagree	"class AnEnum(enum.IntEnum):
  A = 123
  B = 456"
18	wtholke	disagree	"def required_devices(num_devices_required):
  """"""Helper to skip benchmarks that require more devices.""""""
  def helper1(f):
    @functools.wraps(f)
    def helper2(state):
      if jax.device_count() < num_devices_required:
        state.skip_with_error(f""requires {num_devices_required} devices"")
        return
      return f(state)
    return helper2
  return helper1"
19	wtholke	disagree	"def create_mesh(shape, axis_names, state):
  size = np.prod(shape)
  if len(jax.devices()) < size:
    state.skip_with_error(f""Requires {size} devices"")
    return None
  devices = sorted(jax.devices(), key=lambda d: d.id)
  mesh_devices = np.array(devices[:size]).reshape(shape)
  global_mesh = jax.sharding.Mesh(mesh_devices, axis_names)
  return global_mesh"
20	wtholke	4	"def is_windows():
  return sys.platform.startswith(""win32"")"
21	wtholke	4	"def shell(cmd):
  try:
    output = subprocess.check_output(cmd)
  except subprocess.CalledProcessError as e:
    print(e.output)
    raise
  return output.decode(""UTF-8"").strip()"
22	wtholke	5	"def get_python_bin_path(python_bin_path_flag):
  """"""Returns the path to the Python interpreter to use.""""""
  path = python_bin_path_flag or sys.executable
  return path.replace(os.sep, ""/"")"
23	wtholke	4	"def get_python_version(python_bin_path):
  version_output = shell(
    [python_bin_path, ""-c"",
     (""import sys; print(\""{}.{}\"".format(sys.version_info[0], ""
      ""sys.version_info[1]))"")])
  major, minor = map(int, version_output.split("".""))
  return major, minor"
24	wtholke	disagree	"def check_python_version(python_version):
  if python_version < (3, 8):
    print(""ERROR: JAX requires Python 3.8 or newer, found "", python_version)
    sys.exit(-1)"
25	wtholke	disagree	"def get_benchmark_fn(nargs, nshards):
  pmap_fn = pmap(lambda *args: jnp.sum(jnp.array(args)))
  shape = (nshards, 4)
  args = [np.random.random(shape) for _ in range(nargs)]
  sharded_args = pmap(lambda x: x)(args)
  assert all(isinstance(arg, jax.Array) for arg in sharded_args)
  def benchmark_fn():
    for _ in range(100):
      pmap_fn(*sharded_args)
  return benchmark_fn"
26	wtholke	disagree	"def benchmark_fn():
  for _ in range(100):
    pmap_fn(*sharded_args)
return benchmark_fn"
27	wtholke	disagree	"def get_benchmark_fn(nargs, nshards):
  pmap_fn = pmap(lambda *args: jnp.sum(jnp.array(args)))
  shape = (nshards, 4)
  args = [jnp.array(np.random.random(shape)) for _ in range(nargs)]
  assert all(isinstance(arg, jax.Array) for arg in args)
  def benchmark_fn():
    for _ in range(10):
      pmap_fn(*args)
  return benchmark_fn"
28	wtholke	disagree	"def main(logfile: str, outmd: str, outjson: str, name: str):
    print(f""Extracting content of {logfile}"")
    print(f""and writing to {outmd} and {outjson}"")"
29	wtholke	disagree	"class MakeCutouts(nn.Module):
  def __init__(self, cut_size, cut_power=1.0):
      super().__init__()"
30	wtholke	4	"class CLIPGuidedStableDiffusion(DiffusionPipeline):
    """"""CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000
    - https://github.com/Jack000/glid-3-xl
    - https://github.dev/crowsonkb/k-diffusion
    """"""

    def __init__(self, cut_size, cut_power=1.0):
        super().__init__()

    def forward(self, pixel_values, num_cutouts):
    sideY, sideX = pixel_values.shape[2:4]
    max_size = min(sideX, sideY)
    min_size = min(sideX, sideY, self.cut_size)
    cutouts = []
    for _ in range(num_cutouts):
        size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)
        offsetx = torch.randint(0, sideX - size + 1, ())
        offsety = torch.randint(0, sideY - size + 1, ())
        cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]
        cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))
    return torch.cat(cutouts)

    def spherical_dist_loss(x, y):
    x = F.normalize(x, dim=-1)
    y = F.normalize(y, dim=-1)
    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)"
31	wtholke	disagree	"class DDIMNoiseComparativeAnalysisPipeline(DiffusionPipeline):
    r""""""
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)
    """""""
32	wtholke	4	"def preprocess(image):
    if isinstance(image, torch.Tensor):
        return image
    elif isinstance(image, PIL.Image.Image):
        image = [image]"
33	wtholke	disagree	"def check_inputs(self, strength):
    if strength < 0 or strength > 1:
        raise ValueError(f""The value of strength should in [0.0, 1.0] but is {strength}"")"
34	wtholke	disagree	"def get_timesteps(self, num_inference_steps, strength, device):
    # get the original timestep using init_timestep
    init_timestep = min(int(num_inference_steps * strength), num_inference_steps)"
35	wtholke	disagree	"class CheckpointMergerPipeline(DiffusionPipeline):
    """"""
    A class that that supports merging diffusion models based on the discussion here:
    https://github.com/huggingface/diffusers/issues/877

    def __init__(self):
        self.register_to_config()
        super().__init__()

    def _compare_model_configs(self, dict0, dict1):
        if dict0 == dict1:
            return True
        else:
            config0, meta_keys0 = self._remove_meta_keys(dict0)
            config1, meta_keys1 = self._remove_meta_keys(dict1)
            if config0 == config1:
                print(f""Warning !: Mismatch in keys {meta_keys0} and {meta_keys1}."")
                return True
        return False

    def _remove_meta_keys(self, config_dict: Dict):
        meta_keys = []
        temp_dict = config_dict.copy()
        for key in config_dict.keys():
            if key.startswith(""_""):
                temp_dict.pop(key)
                meta_keys.append(key)
        return (temp_dict, meta_keys)"
36	wtholke	disagree	"class ImagicStableDiffusionPipeline(DiffusionPipeline):
    r""""""
    Pipeline for imagic image editing.
    See paper here: https://arxiv.org/pdf/2210.09276.pdf
    """""""
37	wtholke	4	"def preprocess(image):
    w, h = image.size
    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32
    image = image.resize((w, h), resample=PIL_INTERPOLATION[""lanczos""])
    image = np.array(image).astype(np.float32) / 255.0
    image = image[None].transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return 2.0 * image - 1.0"
38	wtholke	4	"def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = ""auto""):
    r""""""
    Enable sliced attention computation.
    When this option is enabled, the attention module will split the input tensor in slices, to compute attention
    in several steps. This is useful to save some memory in exchange for a small speed decrease.
    Args:
        slice_size (`str` or `int`, *optional*, defaults to `""auto""`):
            When `""auto""`, halves the input to the attention heads, so attention will be computed in two steps. If
            a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,
            `attention_head_dim` must be a multiple of `slice_size`.
    """"""
    if slice_size == ""auto"":
        # half the attention head size is usually a good trade-off between
        # speed and memory
        slice_size = self.unet.config.attention_head_dim // 2
    self.unet.set_attention_slice(slice_size)"
39	wtholke	disagree	"def disable_attention_slicing(self):
    r""""""
    Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go
    back to computing attention in one step.
    """"""
    # set slice_size = `None` to disable `attention slicing`
    self.enable_attention_slicing(None)"
40	wtholke	disagree	"class ComposableStableDiffusionPipeline(DiffusionPipeline):
    r""""""
    Pipeline for text-to-image generation using Stable Diffusion."
41	wtholke	4	"def __init__(
    self,
    vae: AutoencoderKL,
    text_encoder: CLIPTextModel,
    tokenizer: CLIPTokenizer,
    unet: UNet2DConditionModel,
    scheduler: Union[
        DDIMScheduler,
        PNDMScheduler,
        LMSDiscreteScheduler,
        EulerDiscreteScheduler,
        EulerAncestralDiscreteScheduler,
        DPMSolverMultistepScheduler,
    ],
    safety_checker: StableDiffusionSafetyChecker,
    feature_extractor: CLIPFeatureExtractor,
    requires_safety_checker: bool = True,
):
    super().__init__()"
42	wtholke	disagree	"def enable_vae_slicing(self):
    r""""""
    Enable sliced VAE decoding.
    """""""
43	wtholke	4	"def disable_vae_slicing(self):
    r""""""
    Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to
    computing decoding in one step.
    """"""
    self.vae.disable_slicing()"
44	wtholke	4	"def enable_sequential_cpu_offload(self, gpu_id=0):
    r""""""
    Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,
    text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a
    `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.
    """"""
    if is_accelerate_available():
        from accelerate import cpu_offload
    else:
        raise ImportError(""Please install accelerate via `pip install accelerate`"")"
45	wtholke	4	"def prepare_mask_and_masked_image(image, mask):
    image = np.array(image.convert(""RGB""))
    image = image[None].transpose(0, 3, 1, 2)
    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0"
46	wtholke	4	"def check_size(image, height, width):
    if isinstance(image, PIL.Image.Image):
        w, h = image.size
    elif isinstance(image, torch.Tensor):
        *_, h, w = image.shape"
47	wtholke	4	"def overlay_inner_image(image, inner_image, paste_offset: Tuple[int] = (0, 0)):
    inner_image = inner_image.convert(""RGBA"")
    image = image.convert(""RGB"")"
48	wtholke	3	"class BitDiffusion(DiffusionPipeline):
    def __init__(
        self,
        unet: UNet2DConditionModel,
        scheduler: Union[DDIMScheduler, DDPMScheduler],
        bit_scale: Optional[float] = 1.0,
    ):
        super().__init__()
        self.bit_scale = bit_scale
        self.scheduler.step = (
            ddim_bit_scheduler_step if isinstance(scheduler, DDIMScheduler) else ddpm_bit_scheduler_step
        )"
49	wtholke	4	"def decimal_to_bits(x, bits=BITS):
    """"""expects image tensor ranging from 0 to 1, outputs bit tensor ranging from -1 to 1""""""
    device = x.device"
50	wtholke	5	"def bits_to_decimal(x, bits=BITS):
    """"""expects bits from -1 to 1, outputs image tensor from 0 to 1""""""
    device = x.device"
51	wtholke	4	"def ddim_bit_scheduler_step(
    self,
    model_output: torch.FloatTensor,
    timestep: int,
    sample: torch.FloatTensor,
    eta: float = 0.0,
    use_clipped_model_output: bool = True,
    generator=None,
    return_dict: bool = True,
) -> Union[DDIMSchedulerOutput, Tuple]:
    """"""
    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
    process from the learned model outputs (most often the predicted noise).
    Args:
        model_output (`torch.FloatTensor`): direct output from learned diffusion model.
        timestep (`int`): current discrete timestep in the diffusion chain.
        sample (`torch.FloatTensor`):
            current instance of sample being created by diffusion process.
        eta (`float`): weight of noise for added noise in diffusion step.
        use_clipped_model_output (`bool`): TODO
        generator: random number generator.
        return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class
    Returns:
        [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:
        [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When
        returning a tuple, the first element is the sample tensor.
    """"""
    if self.num_inference_steps is None:
        raise ValueError(
            ""Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler""
        )"
52	wtholke	4	"def ddpm_bit_scheduler_step(
    self,
    model_output: torch.FloatTensor,
    timestep: int,
    sample: torch.FloatTensor,
    prediction_type=""epsilon"",
    generator=None,
    return_dict: bool = True,
) -> Union[DDPMSchedulerOutput, Tuple]:
    """"""
    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
    process from the learned model outputs (most often the predicted noise).
    Args:
        model_output (`torch.FloatTensor`): direct output from learned diffusion model.
        timestep (`int`): current discrete timestep in the diffusion chain.
        sample (`torch.FloatTensor`):
            current instance of sample being created by diffusion process.
        prediction_type (`str`, default `epsilon`):
            indicates whether the model predicts the noise (epsilon), or the samples (`sample`).
        generator: random number generator.
        return_dict (`bool`): option for returning tuple rather than DDPMSchedulerOutput class
    Returns:
        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] or `tuple`:
        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When
        returning a tuple, the first element is the sample tensor.
    """"""
    t = timestep"
53	wtholke	4	"def set_seed(seed: int):
    hf_set_seed(seed)"
54	wtholke	disagree	"class Entity:
    """"""
    Internal class to represent entities while converting biomedical NER corpora to a standardized format
    (only used for pre-processing purposes!). Each entity consists of the char span it addresses in
    the original text as well as the type of entity (e.g. Chemical, Gene, and so on).
    """""""
55	wtholke	disagree	"class DpEntry(NamedTuple):
    position_end: int
    entity_count: int
    entity_lengths_sum: int
    last_entity: Optional[Entity]"
56	wtholke	disagree	"class ClassificationCorpus(Corpus):
    """"""
    A classification corpus from FastText-formatted text files.
    """""""
57	wtholke	disagree	"class ClassificationDataset(FlairDataset):
    """"""
    Dataset for classification instantiated from a single FastText-formatted file.
    """""""
58	wtholke	disagree	"class CSVClassificationCorpus(Corpus):
    """"""
    Classification corpus instantiated from CSV data files.
    """""""
59	wtholke	disagree	"class CSVClassificationDataset(FlairDataset):
    """"""
    Dataset for text classification from CSV column formatted data.
    """""""
60	wtholke	disagree	"def main():
    print(""#### Versions:"")
    print(f""##### Flair\n{flair.__version__}"")
    print(f""##### Pytorch\n{torch.__version__}"")
    print(f""##### Transformers\n{transformers.__version__}"")
    print(f""#### GPU\n{torch.cuda.is_available()}"")"
61	wtholke	5	"class DataLoader(torch.utils.data.dataloader.DataLoader):
    def __init__(
        self,
        dataset,
        batch_size=1,
        shuffle=False,
        sampler=None,
        batch_sampler=None,
        num_workers=None,
        drop_last=False,
        timeout=0,
        worker_init_fn=None,
    ):
        # in certain cases, multi-CPU data loading makes no sense and slows
        # everything down. For this reason, we detect if a dataset is in-memory:
        # if so, num_workers is set to 0 for faster processing
        flair_dataset = dataset
        while True:
            if type(flair_dataset) is Subset:
                flair_dataset = flair_dataset.dataset
            elif type(flair_dataset) is ConcatDataset:
                flair_dataset = flair_dataset.datasets[0]
            else:
                break"
62	wtholke	disagree	"class FlairDatapointDataset(FlairDataset, Generic[DT]):
    """"""
    A simple Dataset object to wrap a List of Datapoints, for example Sentences
    """""""
63	wtholke	disagree	"class SentenceDataset(FlairDatapointDataset):
    @deprecated(version=""0.11"", reason=""The 'SentenceDataset' class was renamed to 'FlairDatapointDataset'"")
    def __init__(self, sentences: Union[Sentence, List[Sentence]]):
        super().__init__(sentences)"
64	wtholke	disagree	"class ModelArguments:
    model_name_or_path: str = field(
        metadata={""help"": ""The model checkpoint for weights initialization.""},
    )
    layers: str = field(default=""-1"", metadata={""help"": ""Layers to be fine-tuned.""})
    subtoken_pooling: str = field(
        default=""first"",
        metadata={""help"": ""Subtoken pooling strategy used for fine-tuned.""},
    )
    hidden_size: int = field(default=256, metadata={""help"": ""Hidden size for NER model.""})
    use_crf: bool = field(default=False, metadata={""help"": ""Whether to use a CRF on-top or not.""})"
65	wtholke	disagree	"class TrainingArguments:
    num_epochs: int = field(default=10, metadata={""help"": ""The number of training epochs.""})
    batch_size: int = field(default=8, metadata={""help"": ""Batch size used for training.""})
    mini_batch_chunk_size: int = field(
        default=1,
        metadata={""help"": ""If smaller than batch size, batches will be chunked.""},
    )
    learning_rate: float = field(default=5e-05, metadata={""help"": ""Learning rate""})
    seed: int = field(default=42, metadata={""help"": ""Seed used for reproducible fine-tuning results.""})
    device: str = field(default=""cuda:0"", metadata={""help"": ""CUDA device string.""})
    weight_decay: float = field(default=0.0, metadata={""help"": ""Weight decay for optimizer.""})
    embeddings_storage_mode: str = field(default=""none"", metadata={""help"": ""Defines embedding storage method.""})"
66	wtholke	disagree	"class FlertArguments:
    context_size: int = field(default=0, metadata={""help"": ""Context size when using FLERT approach.""})
    respect_document_boundaries: bool = field(
        default=False,
        metadata={""help"": ""Whether to respect document boundaries or not when using FLERT.""},
    )"
67	wtholke	disagree	"class DataArguments:
    dataset_name: str = field(metadata={""help"": ""Flair NER dataset name.""})
    dataset_arguments: str = field(default="""", metadata={""help"": ""Dataset arguments for Flair NER dataset.""})
    output_dir: str = field(
        default=""resources/taggers/ner"",
        metadata={""help"": ""Defines output directory for final fine-tuned model.""},
    )"
68	wtholke	4	"def get_flair_corpus(data_args):
    ner_task_mapping = {}"
69	wtholke	disagree	"class Dictionary:
    """"""
    This class holds a dictionary that maps strings to IDs, used to generate one-hot encodings of strings.
    """""""
70	wtholke	disagree	"class Label:
    """"""
    This class represents a label. Each label has a value and optionally a confidence score. The
    score needs to be between 0.0 and 1.0. Default value for the score is 1.0.
    """""""
71	wtholke	disagree	"class DataPoint:
    """"""
    This is the parent class of all data points in Flair (including Token, Sentence, Image, etc.). Each DataPoint
    must be embeddable (hence the abstract property embedding() and methods to() and clear_embeddings()). Also,
    each DataPoint may have Labels in several layers of annotation (hence the functions add_label(), get_labels()
    and the property 'label')
    """""""
72	wtholke	disagree	"def setup(app: Sphinx) -> None:
    app.add_object_type(
        ""confval"",
        ""confval"",
        objname=""configuration value"",
        indextemplate=""pair: %s; configuration value"",
        doc_field_types=[
            Field(""type"", label=""Type"", has_arg=False, names=(""type"",)),
            Field(""default"", label=""Default"", has_arg=False, names=(""default"",)),
        ],
    )"
73	wtholke	4	"def make_cache(input_dir: str, sqlite: bool) -> MetadataStore:
    if sqlite:
        return SqliteMetadataStore(input_dir)
    else:
        return FilesystemMetadataStore(input_dir)"
74	wtholke	4	"def apply_diff(cache_dir: str, diff_file: str, sqlite: bool = False) -> None:
    cache = make_cache(cache_dir, sqlite)
    with open(diff_file) as f:
        diff = json.load(f)"
75	wtholke	disagree	"def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(""--sqlite"", action=""store_true"", default=False, help=""Use a sqlite cache"")
    parser.add_argument(""cache_dir"", help=""Directory for the cache"")
    parser.add_argument(""diff"", help=""Cache diff file"")
    args = parser.parse_args()"
76	wtholke	4	"class CacheData:
    def __init__(
        self,
        filename: str,
        data_json: JsonDict,
        meta_json: JsonDict,
        data_size: int,
        meta_size: int,
    ) -> None:
        self.filename = filename
        self.data = data_json
        self.meta = meta_json
        self.data_size = data_size
        self.meta_size = meta_size"
77	wtholke	4	"def total_size(self) -> int:
    return self.data_size + self.meta_size"
78	wtholke	4	"def extract_classes(chunks: Iterable[CacheData]) -> Iterable[JsonDict]:
    def extract(chunks: Iterable[JsonDict]) -> Iterable[JsonDict]:
        for chunk in chunks:
            if isinstance(chunk, dict):
                yield chunk
                yield from extract(chunk.values())
            elif isinstance(chunk, list):
                yield from extract(chunk)"
79	wtholke	4	"def extract(chunks: Iterable[JsonDict]) -> Iterable[JsonDict]:
    for chunk in chunks:
        if isinstance(chunk, dict):
            yield chunk
            yield from extract(chunk.values())
        elif isinstance(chunk, list):
            yield from extract(chunk)"
80	wtholke	disagree	"class It(Iterator[str]):
    stop = False"
81	wtholke	4	"class Aw(Awaitable[int]):
    def __await__(self) -> Generator[str, Any, int]:
        yield ""a""
        return 1"
82	wtholke	4	"def plain_generator() -> Generator[str, None, int]:
    yield ""a""
    return 1"
83	wtholke	4	"async def plain_coroutine() -> int:
    return 1"
84	wtholke	4	"def decorated_generator() -> Generator[str, None, int]:
    yield ""a""
    return 1"
85	wtholke	4	"def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--to-sqlite"",
        action=""store_true"",
        default=False,
        help=""Convert to a sqlite cache (default: convert from)"",
    )
    parser.add_argument(
        ""--output_dir"",
        action=""store"",
        default=None,
        help=""Output cache location (default: same as input)"",
    )
    parser.add_argument(""input_dir"", help=""Input directory for the cache"")
    args = parser.parse_args()"
86	wtholke	4	"def pytest_configure(config):
    mypy_source_root = os.path.dirname(os.path.abspath(__file__))
    if os.getcwd() != mypy_source_root:
        os.chdir(mypy_source_root)"
87	wtholke	3	"def pytest_addoption(parser) -> None:
    parser.addoption(
        ""--bench"", action=""store_true"", default=False, help=""Enable the benchmark test runs""
    )"
88	wtholke	disagree	"def parse_commit_title(diff: str) -> str:
    m = re.search(""\n    ([^ ].*)"", diff)
    assert m is not None, ""Could not parse diff""
    return m.group(1)"
89	wtholke	disagree	"def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--typeshed-dir"", help=""location of typeshed"", metavar=""dir"", required=True
    )
    parser.add_argument(""commit"", help=""typeshed commit hash to cherry-pick"")
    args = parser.parse_args()
    typeshed_dir = args.typeshed_dir
    commit = args.commit"
90	wtholke	4	"def setup(app):
    app.add_css_file(""custom.css"")"
91	wtholke	disagree	"class PipenvGroup(DYMMixin, Group):
    """"""Custom Group class provides formatted main help"""""""
92	wtholke	4	"class State:
    def __init__(self):
        self.index = None
        self.verbose = False
        self.quiet = False
        self.pypi_mirror = None
        self.python = None
        self.site_packages = None
        self.clear = False
        self.system = False
        self.project = Project()
        self.installstate = InstallState()
        self.lockoptions = LockOptions()"
93	wtholke	4	"class InstallState:
    def __init__(self):
        self.dev = False
        self.pre = False
        self.selective_upgrade = False
        self.keep_outdated = False
        self.skip_lock = False
        self.ignore_pipfile = False
        self.code = False
        self.requirementstxt = None
        self.deploy = False
        self.packages = []
        self.editables = []
        self.extra_pip_args = []
        self.categories = []"
94	wtholke	4	"class LockOptions:
    def __init__(self):
        self.dev_only = False"
95	wtholke	4	"def cli(
    ctx,
    state,
    where=False,
    venv=False,
    py=False,
    envs=False,
    rm=False,
    bare=False,
    man=False,
    support=None,
    help=False,
    site_packages=None,
    **kwargs,
):
    from pipenv.patched.pip._vendor import rich
    from pipenv.utils.shell import system_which"
96	wtholke	disagree	"def install(state, **kwargs):
    """"""Installs provided packages and adds them to Pipfile, or (if no packages are given), installs all packages from Pipfile.""""""
    from pipenv.routines.install import do_install
    "
97	wtholke	disagree	"def upgrade(state, **kwargs):
    from pipenv.routines.update import upgrade
    from pipenv.utils.project import ensure_project"
98	wtholke	disagree	"def uninstall(ctx, state, all_dev=False, all=False, **kwargs):
    """"""Uninstalls a provided package and removes it from Pipfile.""""""
    from pipenv.routines.uninstall import do_uninstall"
99	wtholke	disagree	"def lock(ctx, state, **kwargs):
    """"""Generates Pipfile.lock.""""""
    from pipenv.routines.lock import do_lock
    from pipenv.utils.project import ensure_project"
100	wtholke	4	"def determine_pip_install_arguments():
    implicit_pip = True
    implicit_setuptools = False
    implicit_wheel = True"
101	wtholke	disagree	"def monkeypatch_for_cert(tmpdir):
    """"""Patches `pip install` to provide default certificate with the lowest priority.
    """""""
102	wtholke	disagree	"def cert_parse_args(self, args):
    if not self.parser.get_default_values().cert:
        # There are no user provided cert -- force use of bundled cert
        self.parser.defaults[""cert""] = cert_path  # calculated above
    return install_parse_args(self, args)"
103	wtholke	disagree	"def bootstrap(tmpdir):
    monkeypatch_for_cert(tmpdir)"
104	wtholke	disagree	"def map(dataset: datasets.Dataset, **kwargs):
    _ = dataset.map(**kwargs)"
105	wtholke	disagree	"def filter(dataset: datasets.Dataset, **kwargs):
    _ = dataset.filter(**kwargs"
106	wtholke	disagree	"def benchmark_map_filter():
    times = {""num examples"": SPEED_TEST_N_EXAMPLES}
    with tempfile.TemporaryDirectory() as tmp_dir:
        features = datasets.Features({""text"": datasets.Value(""string""), ""numbers"": datasets.Value(""float32"")})
        dataset = generate_example_dataset(
            os.path.join(tmp_dir, ""dataset.arrow""), features, num_examples=SPEED_TEST_N_EXAMPLES
        )"
107	wtholke	4	"def tokenize(examples):
    return tokenizer(examples[""text""])"
108	wtholke	4	"class RandIter:
    low: int
    high: int
    size: int
    seed: int"
109	wtholke	disagree	"def generate_100B_dataset(num_examples: int, chunk_size: int) -> datasets.Dataset:
    table = pa.Table.from_pydict({""col"": [0] * chunk_size})
    table = pa.concat_tables([table] * (num_examples // chunk_size))
    return datasets.Dataset(table, fingerprint=""table_100B"")"
110	wtholke	disagree	"def __post_init__(self):
    rng = np.random.default_rng(self.seed)
    self._sampled_values = rng.integers(low=self.low, high=self.high, size=self.size).tolist()"
111	wtholke	4	"def __iter__(self):
    return iter(self._sampled_values)"
112	wtholke	4	"def __len__(self):
    return self.size"
113	wtholke	4	"def format_json_to_md(input_json_file, output_md_file):
    with open(input_json_file, encoding=""utf-8"") as f:
        results = json.load(f)"
114	wtholke	4	"def read(dataset: datasets.Dataset, length):
    for i in range(length):
        _ = dataset[i]"
115	wtholke	4	"def read_batch(dataset: datasets.Dataset, length, batch_size):
    for i in range(0, len(dataset), batch_size):
        _ = dataset[i : i + batch_size]"
116	wtholke	4	"def read_formatted(dataset: datasets.Dataset, length, type):
    with dataset.formatted_as(type=type):
        for i in range(length):
            _ = dataset[i]"
117	wtholke	4	"def read_formatted_batch(dataset: datasets.Dataset, length, batch_size, type):
    with dataset.formatted_as(type=type):
        for i in range(0, length, batch_size):
            _ = dataset[i : i + batch_size]"
118	wtholke	disagree	"def benchmark_iterating():
    times = {""num examples"": SPEED_TEST_N_EXAMPLES}
    functions = [
        (read, {""length"": SMALL_TEST}),
        (read, {""length"": SPEED_TEST_N_EXAMPLES}),
        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 10}),
        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 100}),
        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 1_000}),
        (read_formatted, {""type"": ""numpy"", ""length"": SMALL_TEST}),
        (read_formatted, {""type"": ""pandas"", ""length"": SMALL_TEST}),
        (read_formatted, {""type"": ""torch"", ""length"": SMALL_TEST}),
        (read_formatted, {""type"": ""tensorflow"", ""length"": SMALL_TEST}),
        (read_formatted_batch, {""type"": ""numpy"", ""length"": SMALL_TEST, ""batch_size"": 10}),
        (read_formatted_batch, {""type"": ""numpy"", ""length"": SMALL_TEST, ""batch_size"": 1_000}),
    ]"
119	wtholke	disagree	"def get_duration(func):
    def wrapper(*args, **kwargs):
        starttime = timeit.default_timer()
        _ = func(*args, **kwargs)
        delta = timeit.default_timer() - starttime
        return delta"
120	wtholke	disagree	"def wrapper(*args, **kwargs):
    starttime = timeit.default_timer()
    _ = func(*args, **kwargs)
    delta = timeit.default_timer() - starttime
    return delta"
121	wtholke	disagree	"def generate_examples(features: dict, num_examples=100, seq_shapes=None):
    dummy_data = []
    seq_shapes = seq_shapes or {}
    for i in range(num_examples):
        example = {}
        for col_id, (k, v) in enumerate(features.items()):
            if isinstance(v, _ArrayXD):
                data = np.random.rand(*v.shape).astype(v.dtype)
            elif isinstance(v, datasets.Value):
                if v.dtype == ""string"":
                    data = ""The small grey turtle was surprisingly fast when challenged.""
                else:
                    data = np.random.randint(10, size=1).astype(v.dtype).item()
            elif isinstance(v, datasets.Sequence):
                while isinstance(v, datasets.Sequence):
                    v = v.feature
                shape = seq_shapes[k]
                data = np.random.rand(*shape).astype(v.dtype)
            example[k] = data"
122	wtholke	3	"def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):
    dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)"
123	wtholke	3	"def write(my_features, dummy_data, tmp_dir):
    with ArrowWriter(features=my_features, path=os.path.join(tmp_dir, ""beta.arrow"")) as writer:
        for key, record in dummy_data:
            example = my_features.encode_example(record)
            writer.write(example)
        num_examples, num_bytes = writer.finalize()"
124	wtholke	3	"def read_unformated(feats, tmp_dir):
    dataset = datasets.Dataset.from_file(
        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)
    )
    for _ in dataset:
        pass"
125	wtholke	3	"def read_formatted_as_numpy(feats, tmp_dir):
    dataset = datasets.Dataset.from_file(
        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)
    )
    dataset.set_format(""numpy"")
    for _ in dataset:
        pass"
126	wtholke	3	"def read_batch_unformated(feats, tmp_dir):
    batch_size = 10
    dataset = datasets.Dataset.from_file(
        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)
    )
    for i in range(0, len(dataset), batch_size):
        _ = dataset[i : i + batch_size]"
127	wtholke	3	"def read_batch_formatted_as_numpy(feats, tmp_dir):
    batch_size = 10
    dataset = datasets.Dataset.from_file(
        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)
    )
    dataset.set_format(""numpy"")
    for i in range(0, len(dataset), batch_size):
        _ = dataset[i : i + batch_size]"
128	wtholke	disagree	"def select(dataset: datasets.Dataset):
    _ = dataset.select(range(0, len(dataset), 2))"
129	wtholke	disagree	"def sort(dataset: datasets.Dataset):
    _ = dataset.sort(""numbers"")"
130	wtholke	disagree	"def shuffle(dataset: datasets.Dataset):
    _ = dataset.shuffle()"
131	wtholke	disagree	"def train_test_split(dataset: datasets.Dataset):
    _ = dataset.train_test_split(0.1)"
132	wtholke	disagree	"def shard(dataset: datasets.Dataset, num_shards=10):
    for shard_id in range(num_shards):
        _ = dataset.shard(num_shards, shard_id)"
133	wtholke	disagree	"class RegexpChunkApp:
    """"""
    A graphical tool for exploring the regular expression based chunk
    parser ``nltk.chunk.RegexpChunkParser``.
    """""""
134	wtholke	5	"def normalize_grammar(self, grammar):
    # Strip comments
    grammar = re.sub(r""((\\.|[^#])*)(#.*)?"", r""\1"", grammar)
    # Normalize whitespace
    grammar = re.sub("" +"", "" "", grammar)
    grammar = re.sub(r""\n\s+"", r""\n"", grammar)
    grammar = grammar.strip()
    # [xx] Hack: automatically backslash $!
    grammar = re.sub(r""([^\\])\$"", r""\1\\$"", grammar)
    return grammar"
135	wtholke	disagree	"def _init_bindings(self, top):
    top.bind(""<Control-n>"", self._devset_next)
    top.bind(""<Control-p>"", self._devset_prev)
    top.bind(""<Control-t>"", self.toggle_show_trace)
    top.bind(""<KeyPress>"", self.update)
    top.bind(""<Control-s>"", lambda e: self.save_grammar())
    top.bind(""<Control-o>"", lambda e: self.load_grammar())
    self.grammarbox.bind(""<Control-t>"", self.toggle_show_trace)
    self.grammarbox.bind(""<Control-n>"", self._devset_next)
    self.grammarbox.bind(""<Control-p>"", self._devset_prev)"
136	wtholke	disagree	"def _init_fonts(self, top):
    # TWhat's our font size (default=same as sysfont)
    self._size = IntVar(top)
    self._size.set(20)
    self._font = Font(family=""helvetica"", size=-self._size.get())
    self._smallfont = Font(
        family=""helvetica"", size=-(int(self._size.get() * 14 // 20))
    )"
137	wtholke	disagree	"class RecursiveDescentApp:
    """"""
    A graphical tool for exploring the recursive descent parser.  The tool
    displays the parser's tree and the remaining text, and allows the
    user to control the parser's operation.  In particular, the user
    can expand subtrees on the frontier, match tokens on the frontier
    against the text, and backtrack.  A ""step"" button simply steps
    through the parsing process, performing the operations that
    ``RecursiveDescentParser`` would use.
    """"""

    def __init__(self, grammar, sent, trace=0):
        self._sent = sent
        self._parser = SteppingRecursiveDescentParser(grammar, trace)

"
138	wtholke	4	"def __init__(self, grammar, sent, trace=0):
    self._sent = sent
    self._parser = SteppingRecursiveDescentParser(grammar, trace)"
139	wtholke	5	"def _init_fonts(self, root):
    # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
    self._sysfont = Font(font=Button()[""font""])
    root.option_add(""*Font"", self._sysfont)"
140	wtholke	disagree	"def _init_bindings(self):
    # Key bindings are a good thing.
    self._top.bind(""<Control-q>"", self.destroy)
    self._top.bind(""<Control-x>"", self.destroy)
    self._top.bind(""<Escape>"", self.destroy)
    self._top.bind(""e"", self.expand)
    # self._top.bind('<Alt-e>', self.expand)
    # self._top.bind('<Control-e>', self.expand)
    self._top.bind(""m"", self.match)
    self._top.bind(""<Alt-m>"", self.match)
    self._top.bind(""<Control-m>"", self.match)
    self._top.bind(""b"", self.backtrack)
    self._top.bind(""<Alt-b>"", self.backtrack)
    self._top.bind(""<Control-b>"", self.backtrack)
    self._top.bind(""<Control-z>"", self.backtrack)
    self._top.bind(""<BackSpace>"", self.backtrack)
    self._top.bind(""a"", self.autostep)
    # self._top.bind('<Control-a>', self.autostep)
    self._top.bind(""<Control-space>"", self.autostep)
    self._top.bind(""<Control-c>"", self.cancel_autostep)
    self._top.bind(""<space>"", self.step)
    self._top.bind(""<Delete>"", self.reset)
    self._top.bind(""<Control-p>"", self.postscript)
    # self._top.bind('<h>', self.help)
    # self._top.bind('<Alt-h>', self.help)
    self._top.bind(""<Control-h>"", self.help)
    self._top.bind(""<F1>"", self.help)
    # self._top.bind('<g>', self.toggle_grammar)
    # self._top.bind('<Alt-g>', self.toggle_grammar)
    # self._top.bind('<Control-g>', self.toggle_grammar)
    self._top.bind(""<Control-g>"", self.edit_grammar)
    self._top.bind(""<Control-t>"", self.edit_sentence)"
141	wtholke	4	"class ConcordanceSearchView:
    _BACKGROUND_COLOUR = ""#FFF""  # white"
142	wtholke	4	"class ConcordanceSearchModel:
    def __init__(self, queue):
        self.queue = queue
        self.CORPORA = _CORPORA
        self.DEFAULT_CORPUS = _DEFAULT
        self.selected_corpus = None
        self.reset_query()
        self.reset_results()
        self.result_count = None
        self.last_sent_searched = 0"
143	wtholke	4	"class LoadCorpus(threading.Thread):
    def __init__(self, name, model):
        threading.Thread.__init__(self)
        self.model, self.name = model, name"
144	wtholke	4	"class SearchCorpus(threading.Thread):
    def __init__(self, model, page, count):
        self.model, self.count, self.page = model, count, page
        threading.Thread.__init__(self)"
145	wtholke	disagree	"class Zone:
    def __init__(self, image, initialField, initialText):
        frm = Frame(root)
        frm.config(background=""white"")
        self.image = PhotoImage(format=""gif"", data=images[image.upper()])
        self.imageDimmed = PhotoImage(format=""gif"", data=images[image])
        self.img = Label(frm)
        self.img.config(borderwidth=0)
        self.img.pack(side=""left"")
        self.fld = Text(frm, **fieldParams)
        self.initScrollText(frm, self.fld, initialField)
        frm = Frame(root)
        self.txt = Text(frm, **textParams)
        self.initScrollText(frm, self.txt, initialText)
        for i in range(2):
            self.txt.tag_config(colors[i], background=colors[i])
            self.txt.tag_config(""emph"" + colors[i], foreground=emphColors[i])"
146	wtholke	disagree	"class FindZone(Zone):
    def addTags(self, m):
        color = next(self.colorCycle)
        self.txt.tag_add(color, ""1.0+%sc"" % m.start(), ""1.0+%sc"" % m.end())
        try:
            self.txt.tag_add(
                ""emph"" + color, ""1.0+%sc"" % m.start(""emph""), ""1.0+%sc"" % m.end(""emph"")
            )
        except:
            pass"
147	wtholke	disagree	"class ReplaceZone(Zone):
    def addTags(self, m):
        s = sz.rex.sub(self.repl, m.group())
        self.txt.delete(
            ""1.0+%sc"" % (m.start() + self.diff), ""1.0+%sc"" % (m.end() + self.diff)
        )
        self.txt.insert(""1.0+%sc"" % (m.start() + self.diff), s, next(self.colorCycle))
        self.diff += len(s) - (m.end() - m.start())"
148	wtholke	4	"def __init__(self, image, initialField, initialText):
    frm = Frame(root)
    frm.config(background=""white"")
    self.image = PhotoImage(format=""gif"", data=images[image.upper()])
    self.imageDimmed = PhotoImage(format=""gif"", data=images[image])
    self.img = Label(frm)
    self.img.config(borderwidth=0)
    self.img.pack(side=""left"")
    self.fld = Text(frm, **fieldParams)
    self.initScrollText(frm, self.fld, initialField)
    frm = Frame(root)
    self.txt = Text(frm, **textParams)
    self.initScrollText(frm, self.txt, initialText)
    for i in range(2):
        self.txt.tag_config(colors[i], background=colors[i])
        self.txt.tag_config(""emph"" + colors[i], foreground=emphColors[i])"
149	wtholke	4	"def initScrollText(self, frm, txt, contents):
    scl = Scrollbar(frm)
    scl.config(command=txt.yview)
    scl.pack(side=""right"", fill=""y"")
    txt.pack(side=""left"", expand=True, fill=""x"")
    txt.config(yscrollcommand=scl.set)
    txt.insert(""1.0"", contents)
    frm.pack(fill=""x"")
    Frame(height=2, bd=1, relief=""ridge"").pack(fill=""x"")"
150	wtholke	4	"class CollocationsView:
    _BACKGROUND_COLOUR = ""#FFF""  # white"
151	wtholke	4	"class CollocationsModel:
    def __init__(self, queue):
        self.result_count = None
        self.selected_corpus = None
        self.collocations = None
        self.CORPORA = _CORPORA
        self.DEFAULT_CORPUS = _DEFAULT
        self.queue = queue
        self.reset_results()"
152	wtholke	4	"class LoadCorpus(threading.Thread):
    def __init__(self, name, model):
        threading.Thread.__init__(self)
        self.model, self.name = model, name"
153	wtholke	4	"def __init__(self):
    self.queue = q.Queue()
    self.model = CollocationsModel(self.queue)
    self.top = Tk()
    self._init_top(self.top)
    self._init_menubar()
    self._init_widgets(self.top)
    self.load_corpus(self.model.DEFAULT_CORPUS)
    self.after = self.top.after(POLL_INTERVAL, self._poll)"
154	wtholke	4	"class EdgeList(ColorizedList):
    ARROW = SymbolWidget.SYMBOLS[""rightarrow""]"
155	wtholke	disagree	"class ChartMatrixView:
    """"""
    A view of a chart that displays the contents of the corresponding matrix.
    """""""
156	wtholke	4	"class ChartResultsView:
    def __init__(self, parent, chart, grammar, toplevel=True):
        self._chart = chart
        self._grammar = grammar
        self._trees = []
        self._y = 10
        self._treewidgets = []
        self._selection = None
        self._selectbox = None"
157	wtholke	disagree	"class ChartView:
    """"""
    A component for viewing charts.  This is used by ``ChartParserApp`` to
    allow students to interactively experiment with various chart
    parsing techniques.  It is also used by ``Chart.draw()``.
    """""""
158	wtholke	disagree	"def _fake_PIPE(*args, **kwargs):
    raise NotImplementedError(""subprocess.PIPE is not supported."")"
159	wtholke	disagree	"def _fake_Popen(*args, **kwargs):
    raise NotImplementedError(""subprocess.Popen is not supported."")"
160	wtholke	disagree	"def demo():
    print(""To run the demo code for a module, type nltk.module.demo()"")"
161	wtholke	disagree	"def make_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""imminent"",
    )
    parser.add_argument(""--postgres"", action=""store_true"")
    parser.add_argument(""--elasticsearch5"", action=""store_true"")
    parser.add_argument(""--elasticsearch6"", action=""store_true"")
    parser.add_argument(""--elasticsearch7"", action=""store_true"")
    parser.add_argument(""--emailuser"", action=""store_true"")
    parser.add_argument(""--disabletimezone"", action=""store_true"")
    parser.add_argument(""--bench"", action=""store_true"")
    return parser"
162	wtholke	3	"def parse_args(args=None):
    return make_parser().parse_known_args(args)"
163	wtholke	disagree	"def runtests():
    args, rest = parse_args()"
164	wtholke	disagree	"def pytest_addoption(parser):
    parser.addoption(
        ""--deprecation"",
        choices=[""all"", ""pending"", ""imminent"", ""none""],
        default=""pending"",
    )
    parser.addoption(""--postgres"", action=""store_true"")
    parser.addoption(""--elasticsearch"", action=""store_true"")"
165	wtholke	4	"def pytest_configure(config):
    deprecation = config.getoption(""deprecation"")"
166	wtholke	4	"def pytest_unconfigure(config):
    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT"
167	wtholke	4	"def get_language_name(locale_string):
    try:
        return LANGUAGE_OVERRIDES[locale_string]
    except KeyError:
        return Locale.parse(locale_string).english_name"
168	wtholke	4	"def setup(app):
    app.add_js_file(""js/banner.js"")"
169	wtholke	5	"def __init__(self, environment):
    """"""Initialize the extension with the given environment.""""""
    super().__init__(environment)"
170	wtholke	5	"def _expand_path(path):
    """"""Expand both environment variables and user home in the given path.""""""
    path = os.path.expandvars(path)
    path = os.path.expanduser(path)
    return path"
171	wtholke	disagree	"def merge_configs(default, overwrite):
    """"""Recursively update a dict with the key/value pair of another.

    def get_config(config_path):
    """"""Retrieve the config from the specified path, returning a config dict.""""""
    if not os.path.exists(config_path):
        raise ConfigDoesNotExistException(f'Config file {config_path} does not exist.')"
172	wtholke	disagree	"def get_config(config_path):
    """"""Retrieve the config from the specified path, returning a config dict.""""""
    if not os.path.exists(config_path):
        raise ConfigDoesNotExistException(f'Config file {config_path} does not exist.')"
173	wtholke	disagree	"class ConfigDoesNotExistException(CookiecutterException):
    """"""
    Exception for missing config file.
    """""""
174	wtholke	disagree	"def version_msg():
    """"""Return the Cookiecutter version, location and Python powering it.""""""
    python_version = sys.version
    location = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    return f""Cookiecutter {__version__} from {location} (Python {python_version})"""
175	wtholke	disagree	"def validate_extra_context(ctx, param, value):
    """"""Validate extra context.""""""
    for string in value:
        if '=' not in string:
            raise click.BadParameter(
                f""EXTRA_CONTEXT should contain items of the form key=value; ""
                f""'{string}' doesn't match that form""
            )"
176	wtholke	disagree	"def list_installed_templates(default_config, passed_config_file):
    """"""List installed (locally cloned) templates. Use cookiecutter --list-installed.""""""
    config = get_user_config(passed_config_file, default_config)
    cookiecutter_folder = config.get('cookiecutters_dir')
    if not os.path.exists(cookiecutter_folder):
        click.echo(
            f""Error: Cannot list installed templates. ""
            f""Folder does not exist: {cookiecutter_folder}""
        )
        sys.exit(-1)"
177	wtholke	disagree	"def main(
    template,
    extra_context,
    no_input,
    checkout,
    verbose,
    replay,
    overwrite_if_exists,
    output_dir,
    config_file,
    default_config,
    debug_file,
    directory,
    skip_if_file_exists,
    accept_hooks,
    replay_file,
    list_installed,
    keep_project_on_failure,
):
    """"""Create a project from a Cookiecutter project template (TEMPLATE)."
178	wtholke	4	"def load_response(url: str, filename: str) -> HtmlResponse:
    input_path = Path(__file__).parent / ""_tests"" / filename
    return HtmlResponse(url, body=input_path.read_bytes())"
179	wtholke	4	"def setup(namespace):
    namespace[""load_response""] = load_response"
180	wtholke	4	"class settingslist_node(nodes.General, nodes.Element):
    pass"
181	wtholke	4	"class SettingsListDirective(Directive):
    def run(self):
        return [settingslist_node("""")]"
182	wtholke	5	"def is_setting_index(node):
    if node.tagname == ""index"" and node[""entries""]:
        # index entries for setting directives look like:
        # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]
        entry_type, info, refid = node[""entries""][0][:3]
        return entry_type == ""pair"" and info.endswith(""; setting"")
    return False"
183	wtholke	5	"def get_setting_target(node):
    # target nodes are placed next to the node in the doc tree
    return node.parent[node.parent.index(node) + 1]"
184	wtholke	4	"def setup(app):
    app.connect(""autodoc-skip-member"", maybe_skip_member)"
185	wtholke	disagree	"def maybe_skip_member(app, what, name, obj, skip, options):
    if not skip:
        # autodocs was generating a text ""alias of"" for the following members
        # https://github.com/sphinx-doc/sphinx/issues/4422
        return name in {""default_item_class"", ""default_selector_class""}
    return skip"
186	wtholke	5	"def main():
    # Used for remembering the file (and its contents)
    # so we don't have to open the same file again.
    _filename = None
    _contents = None"
187	wtholke	4	"class QPSSpider(Spider):
    name = ""qps""
    benchurl = ""http://localhost:8880/"""
188	wtholke	disagree	"def __init__(self, *a, **kw):
    super().__init__(*a, **kw)
    if self.qps is not None:
        self.qps = float(self.qps)
        self.download_delay = 1 / self.qps
    elif self.download_delay is not None:
        self.download_delay = float(self.download_delay)"
189	wtholke	4	"def start_requests(self):
    url = self.benchurl
    if self.latency is not None:
        url += f""?latency={self.latency}"""
190	wtholke	4	"def parse(self, response):
    pass"
191	wtholke	4	"def _py_files(folder):
    return (str(p) for p in Path(folder).rglob(""*.py""))"
192	wtholke	5	"def chdir(tmpdir):
    """"""Change to pytest-provided temporary directory""""""
    tmpdir.chdir()"
193	wtholke	4	"def pytest_addoption(parser):
    parser.addoption(
        ""--reactor"",
        default=""default"",
        choices=[""default"", ""asyncio""],
    )"
194	wtholke	4	"class Root(Resource):
    def __init__(self):
        Resource.__init__(self)
        self.concurrent = 0
        self.tail = deque(maxlen=100)
        self._reset_stats()"
195	wtholke	4	"def __init__(self):
    Resource.__init__(self)
    self.concurrent = 0
    self.tail = deque(maxlen=100)
    self._reset_stats()"
196	wtholke	4	"def _reset_stats(self):
    self.tail.clear()
    self.start = self.lastmark = self.lasttime = time()"
197	wtholke	4	"def getChild(self, request, name):
    return self"
198	wtholke	4	"def render(self, request):
    now = time()
    delta = now - self.lasttime"
199	wtholke	disagree	"def test_dtw(N: int, M: int):
    steps = np.concatenate([np.zeros(N - 1), np.ones(M - 1)])
    np.random.shuffle(steps)
    x = np.random.random((N, M)).astype(np.float32)"
200	wtholke	disagree	"def test_dtw_cuda_equivalence(N: int, M: int):
    x_numpy = np.random.randn(N, M).astype(np.float32)
    x_cuda = torch.from_numpy(x_numpy).cuda()"
201	wtholke	4	"def test_median_filter(shape):
    x = torch.randn(*shape)"
202	wtholke	4	"def test_median_filter_equivalence(shape):
    x = torch.randn(*shape)"
203	wtholke	4	"def test_number_normalizer(std):
    assert std(""two"") == ""2""
    assert std(""thirty one"") == ""31""
    assert std(""five twenty four"") == ""524""
    assert std(""nineteen ninety nine"") == ""1999""
    assert std(""twenty nineteen"") == ""2019"""
204	wtholke	4	"def test_spelling_normalizer():
    std = EnglishSpellingNormalizer()"
205	wtholke	4	"def test_text_normalizer():
    std = EnglishTextNormalizer()
    assert std(""Let's"") == ""let us""
    assert std(""he's like"") == ""he is like""
    assert std(""she's been like"") == ""she has been like""
    assert std(""10km"") == ""10 km""
    assert std(""10mm"") == ""10 mm""
    assert std(""RC232"") == ""rc 232"""
206	wtholke	4	"def test_audio():
    audio_path = os.path.join(os.path.dirname(__file__), ""jfk.flac"")
    audio = load_audio(audio_path)
    assert audio.ndim == 1
    assert SAMPLE_RATE * 10 < audio.shape[0] < SAMPLE_RATE * 12
    assert 0 < audio.std() < 1"
207	wtholke	4	"def pytest_configure(config):
    config.addinivalue_line(""markers"", ""requires_cuda"")"
208	wtholke	4	"def random():
    rand.seed(42)
    numpy.random.seed(42)"
209	wtholke	4	"def test_transcribe(model_name: str):
    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    model = whisper.load_model(model_name).to(device)
    audio_path = os.path.join(os.path.dirname(__file__), ""jfk.flac"")"
210	wtholke	4	"def test_tokenizer():
    gpt2_tokenizer = get_tokenizer(multilingual=False)
    multilingual_tokenizer = get_tokenizer(multilingual=True)"
211	wtholke	4	"def test_split_on_unicode():
    multilingual_tokenizer = get_tokenizer(multilingual=True)"
212	wtholke	4	"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    os.makedirs(root, exist_ok=True)"
213	wtholke	5	"def available_models() -> List[str]:
    """"""Returns the names of available models""""""
    return list(_MODELS.keys())"
214	wtholke	5	"def load_model(
    name: str,
    device: Optional[Union[str, torch.device]] = None,
    download_root: str = None,
    in_memory: bool = False,
) -> Whisper:
    """"""
    Load a Whisper ASR model"
215	wtholke	4	"class _Undefined:
    def __repr__(self) -> str:
        return 'see-below'"
216	wtholke	disagree	"class Snowflake(Protocol):
    """"""An ABC that details the common operations on a Discord model.
    """""""
217	wtholke	disagree	"class User(Snowflake, Protocol):
    """"""An ABC that details the common operations on a Discord user.
    """""""
218	wtholke	disagree	"class PrivateChannel:
    """"""An ABC that details the common operations on a private Discord channel.
    """""""
219	wtholke	4	"class _Overwrites:
    __slots__ = ('id', 'allow', 'deny', 'type')"
220	wtholke	4	"class VersionInfo(NamedTuple):
    major: int
    minor: int
    micro: int
    releaselevel: Literal[""alpha"", ""beta"", ""candidate"", ""final""]
    serial: int"
221	wtholke	disagree	"class Bot(commands.{base}):
    def __init__(self, intents: discord.Intents, **kwargs):
        super().__init__(command_prefix=commands.when_mentioned_or('{prefix}'), intents=intents, **kwargs)"
222	wtholke	disagree	"def show_version() -> None:
    entries = []"
223	wtholke	disagree	"def core(parser: argparse.ArgumentParser, args: argparse.Namespace) -> None:
    if args.version:
        show_version()
    else:
        parser.print_help()"
224	wtholke	disagree	"class Parameter:
    """"""A class that contains the parameter information of a :class:`Command` callback.
    """""""
225	wtholke	disagree	"class ContextMenu:
    """"""A class that implements a context menu application command.
    """""""
226	wtholke	disagree	"class BaseActivity:
    """"""The base activity that all user-settable activities inherit from.
    A user-settable activity is one that can be used in :meth:`Client.change_presence`.
    """""""
227	wtholke	disagree	"class Activity(BaseActivity):
    """"""Represents an activity in Discord.
    """""""
228	wtholke	disagree	"class Game(BaseActivity):
    """"""A slimmed down version of :class:`Activity` that represents a Discord game.
    """""""
229	wtholke	disagree	"class Streaming(BaseActivity):
    """"""A slimmed down version of :class:`Activity` that represents a Discord streaming status.
    """""""
230	wtholke	disagree	"class Spotify:
    """"""Represents a Spotify listening activity from Discord. This is a special case of
    :class:`Activity` that makes it easier to work with the Spotify integration.
    """""""
231	wtholke	disagree	"class Cooldown:
    """"""Represents a cooldown for a command.
    """""""
232	wtholke	4	"def __init__(self, rate: float, per: float) -> None:
    self.rate: int = int(rate)
    self.per: float = float(per)
    self._window: float = 0.0
    self._tokens: int = self.rate
    self._last: float = 0.0"
233	wtholke	disagree	"def get_tokens(self, current: Optional[float] = None) -> int:
    """"""Returns the number of available tokens before rate limiting is applied.
    """""""
234	wtholke	disagree	"def get_retry_after(self, current: Optional[float] = None) -> float:
    """"""Returns the time in seconds until the cooldown will be reset.
    """""""
235	wtholke	disagree	"def update_rate_limit(self, current: Optional[float] = None, *, tokens: int = 1) -> Optional[float]:
    """"""Updates the cooldown rate limit.
    """""""
236	wtholke	4	"def close_api_gracefully(apis):
    try:
        for api in apis.values():
            process = api['process']
            childs = get_child_pids(process.pid)
            for p in childs:
                try:
                    os.kill(p, signal.SIGTERM)
                except Exception:
                    p.kill()
            sys.stdout.flush()
            process.terminate()
            process.join()
            sys.stdout.flush()
    except KeyboardInterrupt:
        sys.exit(0)
    except psutil.NoSuchProcess:
        pass"
237	wtholke	disagree	"async def wait_api_start(api_name, pid, port):
    timeout = 60
    start_time = time.time()
    started = is_pid_listen_port(pid, port)
    while (time.time() - start_time) < timeout and started is False:
        await asyncio.sleep(0.5)
        started = is_pid_listen_port(pid, port)
    return api_name, port, started"
238	wtholke	disagree	"async def wait_apis_start():
    futures = [
        wait_api_start(api_name, api_data['process'].pid, api_data['port'])
        for api_name, api_data in apis.items() if 'port' in api_data
    ]
    for i, future in enumerate(asyncio.as_completed(futures)):
        api_name, port, started = await future
        if started:
            print(f""{api_name} API: started on {port}"")
        else:
            log.logger.error(f""ERROR: {api_name} API cant start on {port}"")"
239	wtholke	4	"def index():
    return ""MindsDB Hanler Discovery"", 200"
240	wtholke	disagree	"def register():
    try:
        params = request.json
        host = params.get(""host"")
        port = params.get(""port"")
        _type = params.get(""type"")
        Cache[(host, port)] = _type
        return ""OK"", 200
    except Exception as e:
        return str(e), 500"
241	wtholke	4	"def discover():
    res = {}
    try:
        for k in Cache:
            _type = Cache[k]
            rec = {""host"": k[0], ""port"": k[1]}
            if _type not in res:
                res[_type] = [rec]
            else:
                res[_type].append(rec)
    except Exception as e:
        return {""error"": str(e)}, 500
    return res, 200"
242	wtholke	disagree	"class QuietSimpleHTTPServer(SimpleHTTPRequestHandler):
    def log_message(self, *args, **kwargs):
        pass"
243	wtholke	4	"class Context:
    benchmarks: ClassVar[List[BaseRunner]] = []
    stack: ExitStack = field(default_factory=ExitStack)
    runner: pyperf.Runner = field(default_factory=pyperf.Runner)"
244	wtholke	disagree	"class BaseRunner:
    """"""
    An individual benchmark case. By default it has the category
    (e.g like startup or download) and a name.
    """""""
245	wtholke	disagree	"class CommandRunner(BaseRunner):
    """"""
    Run a single command, and benchmark it.
    """""""
246	wtholke	4	"def build_binaries() -> Iterator[Tuple[str, Path]]:
    for target_script, extra_args in TARGET_SCRIPTS.items():
        subprocess.check_call(
            [
                'pyinstaller',
                '--onefile',
                '--noupx',
                '-p',
                HTTPIE_DIR,
                '--additional-hooks-dir',
                HOOKS_DIR,
                *extra_args,
                target_script,
            ]
        )"
247	wtholke	disagree	"def build_packages(http_binary: Path, httpie_binary: Path) -> None:
    import httpie"
248	wtholke	4	"def main():
    binaries = dict(build_binaries())
    build_packages(binaries['http_cli'], binaries['httpie_cli'])"
249	wtholke	5	"class FinishedForNow(Exception):
    """"""Raised when remaining GitHub rate limit is zero.""""""

    def main(previous_release: str, current_release: str) -> int:
    since = release_date(previous_release)
    until = release_date(current_release)"
