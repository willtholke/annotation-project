{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Python Code Readability\n",
    "\n",
    "Will Tholke, Alex Truong, Andrew Zhang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes (to be removed before submission)\n",
    "\n",
    "- We're using ordinal regression, basing the model off of [this notebook](https://github.com/dbamman/nlp23/blob/main/AP/OrdinalRegression.ipynb)\n",
    "- The labels 1-5 are considered ordinal because the distance between any two values cannot be compared; the distance between 1 and 2 cannot be compared to the distance between 2 and 3, for example, whereas these tuples of numbers can be compared when they are considered numerical"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk\n",
    "import math\n",
    "from scipy.stats import norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n",
      "  warn(RuntimeWarning(msg))\r\n",
      "[nltk_data] Downloading package punkt to\r\n",
      "[nltk_data]     /Users/willtholke/nltk_data...\r\n",
      "[nltk_data]   Package punkt is already up-to-date!\r\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text.replace('<newline>', '\\n')\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "def load_ordinal_data(filename, ordering):\n",
    "    X = []\n",
    "    Y = []\n",
    "    orig_Y = []\n",
    "    for ordinal in ordering:\n",
    "        Y.append([])\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[2].lstrip().rstrip()\n",
    "            text = cols[3]\n",
    "            clean_text(text)\n",
    "            X.append(text)\n",
    "\n",
    "            index = ordering.index(label)\n",
    "            for i in range(len(ordering)):\n",
    "                if index > i:\n",
    "                    Y[i].append(1)\n",
    "                else:\n",
    "                    Y[i].append(0)\n",
    "            orig_Y.append(label)\n",
    "\n",
    "    return X, Y, orig_Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "outputs": [],
   "source": [
    "class OrdinalClassifier:\n",
    "\n",
    "    def __init__(self, ordinal_values, feature_method, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY):\n",
    "        self.ordinal_values=ordinal_values\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_count=2\n",
    "        self.log_regs = [None] * (len(self.ordinal_values)-1)\n",
    "\n",
    "        self.trainY=trainY\n",
    "        self.devY=devY\n",
    "        self.testY=testY\n",
    "\n",
    "        self.orig_trainY=orig_trainY\n",
    "        self.orig_devY=orig_devY\n",
    "        self.orig_testY=orig_testY\n",
    "\n",
    "        self.trainX = self.process(trainX, training=True)\n",
    "        self.devX = self.process(devX, training=False)\n",
    "        self.testX = self.process(testX, training=False)\n",
    "\n",
    "    # Featurize entire dataset\n",
    "    def featurize(self, data):\n",
    "        featurized_data = []\n",
    "        for text in data:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
    "    def process(self, X_data, training = False):\n",
    "\n",
    "        data = self.featurize(X_data)\n",
    "\n",
    "        if training:\n",
    "            fid = 0\n",
    "            feature_doc_count = Counter()\n",
    "            for feats in data:\n",
    "                for feat in feats:\n",
    "                    feature_doc_count[feat]+= 1\n",
    "\n",
    "            for feat in feature_doc_count:\n",
    "                if feature_doc_count[feat] >= self.min_feature_count:\n",
    "                    self.feature_vocab[feat] = fid\n",
    "                    fid += 1\n",
    "\n",
    "        F = len(self.feature_vocab)\n",
    "        D = len(data)\n",
    "        X = sparse.dok_matrix((D, F))\n",
    "        for idx, feats in enumerate(data):\n",
    "            for feat in feats:\n",
    "                if feat in self.feature_vocab:\n",
    "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def train(self):\n",
    "        (D,F) = self.trainX.shape\n",
    "\n",
    "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
    "            best_dev_accuracy=0\n",
    "            best_model=None\n",
    "            for C in [0.0001, 0.001, 0.1, 1, 5, 10, 50, 100, 1000]:\n",
    "\n",
    "                log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
    "                log_reg.fit(self.trainX, self.trainY[idx]) # ERROR LINE\n",
    "\n",
    "                development_accuracy = log_reg.score(self.devX, self.devY[idx])\n",
    "                if development_accuracy > best_dev_accuracy:\n",
    "                    best_dev_accuracy=development_accuracy\n",
    "                    best_model=log_reg\n",
    "\n",
    "\n",
    "            self.log_regs[idx]=best_model\n",
    "\n",
    "    def test(self):\n",
    "        cor=tot=0\n",
    "        counts=Counter()\n",
    "        preds=[None]*(len(self.ordinal_values)-1)\n",
    "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
    "            preds[idx]=self.log_regs[idx].predict_proba(self.testX)[:,1]\n",
    "\n",
    "        preds=np.array(preds)\n",
    "\n",
    "        for data_point in range(len(preds[0])):\n",
    "\n",
    "\n",
    "            ordinal_preds=np.zeros(len(self.ordinal_values))\n",
    "            for ordinal in range(len(self.ordinal_values)-1):\n",
    "                if ordinal == 0:\n",
    "                    ordinal_preds[ordinal]=1-preds[ordinal][data_point]\n",
    "                else:\n",
    "                    ordinal_preds[ordinal]=preds[ordinal-1][data_point]-preds[ordinal][data_point]\n",
    "\n",
    "            ordinal_preds[len(self.ordinal_values)-1]=preds[len(preds)-1][data_point]\n",
    "\n",
    "            prediction=np.argmax(ordinal_preds)\n",
    "            counts[prediction]+=1\n",
    "            if prediction == self.ordinal_values.index(self.orig_testY[data_point]):\n",
    "                cor+=1\n",
    "            tot+=1\n",
    "\n",
    "        return cor/tot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [],
   "source": [
    "def confidence_intervals(accuracy, n, significance_level):\n",
    "    critical_value=(1-significance_level)/2\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
    "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [],
   "source": [
    "def binary_bow_featurize(text):\n",
    "    feats = {}\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        feats[word]=1\n",
    "\n",
    "    return feats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [],
   "source": [
    "def combiner_function(text):\n",
    "  all_feats={}\n",
    "  for feature in [binary_bow_featurize]:\n",
    "    all_feats.update(feature(text))\n",
    "  return all_feats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "outputs": [],
   "source": [
    "def run(trainingFile, devFile, testFile, ordinal_values):\n",
    "    trainX, trainY, orig_trainY = load_ordinal_data(trainingFile, ordinal_values)\n",
    "    devX, devY, orig_devY = load_ordinal_data(devFile, ordinal_values)\n",
    "    testX, testY, orig_testY = load_ordinal_data(testFile, ordinal_values)\n",
    "\n",
    "    # Check if there is only one class in the data\n",
    "    unique_classes = set(orig_trainY + orig_devY + orig_testY)\n",
    "    if len(unique_classes) == 1:\n",
    "        print(\"Only one class in the data, no need to train model.\")\n",
    "        return\n",
    "\n",
    "    simple_classifier = OrdinalClassifier(ordinal_values, combiner_function, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n",
    "\n",
    "    simple_classifier.train()\n",
    "    accuracy = simple_classifier.test()\n",
    "\n",
    "    lower, upper = confidence_intervals(accuracy, len(testY[0]), .95)\n",
    "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[527], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m testFile \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits/test.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m ordinal_values \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 7\u001B[0m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainingFile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevFile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtestFile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mordinal_values\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[526], line 14\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(trainingFile, devFile, testFile, ordinal_values)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     12\u001B[0m simple_classifier \u001B[38;5;241m=\u001B[39m OrdinalClassifier(ordinal_values, combiner_function, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n\u001B[0;32m---> 14\u001B[0m \u001B[43msimple_classifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m simple_classifier\u001B[38;5;241m.\u001B[39mtest()\n\u001B[1;32m     17\u001B[0m lower, upper \u001B[38;5;241m=\u001B[39m confidence_intervals(accuracy, \u001B[38;5;28mlen\u001B[39m(testY[\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;241m.95\u001B[39m)\n",
      "Cell \u001B[0;32mIn[522], line 66\u001B[0m, in \u001B[0;36mOrdinalClassifier.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m C \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0.0001\u001B[39m, \u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m1000\u001B[39m]:\n\u001B[1;32m     65\u001B[0m     log_reg \u001B[38;5;241m=\u001B[39m linear_model\u001B[38;5;241m.\u001B[39mLogisticRegression(C \u001B[38;5;241m=\u001B[39m C, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m---> 66\u001B[0m     \u001B[43mlog_reg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainY\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# ERROR LINE\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     development_accuracy \u001B[38;5;241m=\u001B[39m log_reg\u001B[38;5;241m.\u001B[39mscore(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevX, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevY[idx])\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m development_accuracy \u001B[38;5;241m>\u001B[39m best_dev_accuracy:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/annotation-project-igSnq0fm/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1241\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1239\u001B[0m classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_\n\u001B[1;32m   1240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_classes \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis solver needs samples of at least 2 classes\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1243\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the data, but the data contains only one\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m class: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1245\u001B[0m         \u001B[38;5;241m%\u001B[39m classes_[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1246\u001B[0m     )\n\u001B[1;32m   1248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m   1249\u001B[0m     n_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "trainingFile = \"splits/train.txt\"\n",
    "devFile = \"splits/dev.txt\"\n",
    "testFile = \"splits/test.txt\"\n",
    "\n",
    "ordinal_values = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "run(trainingFile, devFile, testFile, ordinal_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation: Accuracy & Confidence Intervals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part B: Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The analysis here should communicate what kind of information others should know if they were to use our model as well as aspects of some fundamental concept that you hadn't considered while annotating. We will be graded on the depth of our analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# some code used for analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
