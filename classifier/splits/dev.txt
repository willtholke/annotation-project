450	adjudicated	4	async def apply_deployment(deployment):<newline>    async with prefect.get_client() as client:<newline>        flow_id = await client.create_flow_from_name(deployment.flow_name)<newline>        await client.create_deployment(flow_id=flow_id, name=deployment.name)
392	adjudicated	4	"class _CheckListManager:<newline>    def __init__(<newline>        self,<newline>        task_suite: TaskSuite,<newline>        predictor: Predictor,<newline>        capabilities: Optional[List[str]] = None,<newline>        max_examples: Optional[int] = None,<newline>        output_file: Optional[str] = None,<newline>        print_summary_args: Optional[Dict[str, Any]] = None,<newline>    ) -> None:<newline>        self._task_suite = task_suite<newline>        self._predictor = predictor<newline>        self._capabilities = capabilities<newline>        self._max_examples = max_examples<newline>        self._output_file = None if output_file is None else open(output_file, ""w"")<newline>        self._print_summary_args = print_summary_args or {}"
176	adjudicated	5	"def list_installed_templates(default_config, passed_config_file):<newline>    """"""List installed (locally cloned) templates. Use cookiecutter --list-installed.""""""<newline>    config = get_user_config(passed_config_file, default_config)<newline>    cookiecutter_folder = config.get('cookiecutters_dir')<newline>    if not os.path.exists(cookiecutter_folder):<newline>        click.echo(<newline>            f""Error: Cannot list installed templates. ""<newline>            f""Folder does not exist: {cookiecutter_folder}""<newline>        )<newline>        sys.exit(-1)"
22	adjudicated	5	"def get_python_bin_path(python_bin_path_flag):<newline>  """"""Returns the path to the Python interpreter to use.""""""<newline>  path = python_bin_path_flag or sys.executable<newline>  return path.replace(os.sep, ""/"")"
435	adjudicated	4	"def pytest_configure(config):<newline>    config.addinivalue_line(<newline>        ""markers"", ""is_pt_tf_cross_test: mark test to run only when PT and TF interactions are tested""<newline>    )<newline>    config.addinivalue_line(<newline>        ""markers"", ""is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested""<newline>    )<newline>    config.addinivalue_line(<newline>        ""markers"", ""is_pipeline_test: mark test to run only when pipelines are tested""<newline>    )<newline>    config.addinivalue_line(""markers"", ""is_staging_test: mark test to run only in the staging environment"")"
476	adjudicated	5	"def generate_readmes(templates_path: Path, dest_path: Path):<newline>    """"""Add the code snippet template to the readmes""""""<newline>    readme_templates_path = templates_path / ""models""<newline>    code_template_path = templates_path / ""code_snippets.md"""
127	adjudicated	3	"def read_batch_formatted_as_numpy(feats, tmp_dir):<newline>    batch_size = 10<newline>    dataset = datasets.Dataset.from_file(<newline>        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)<newline>    )<newline>    dataset.set_format(""numpy"")<newline>    for i in range(0, len(dataset), batch_size):<newline>        _ = dataset[i : i + batch_size]"
149	adjudicated	4	"def initScrollText(self, frm, txt, contents):<newline>    scl = Scrollbar(frm)<newline>    scl.config(command=txt.yview)<newline>    scl.pack(side=""right"", fill=""y"")<newline>    txt.pack(side=""left"", expand=True, fill=""x"")<newline>    txt.config(yscrollcommand=scl.set)<newline>    txt.insert(""1.0"", contents)<newline>    frm.pack(fill=""x"")<newline>    Frame(height=2, bd=1, relief=""ridge"").pack(fill=""x"")"
335	adjudicated	5	"async def main(loop):<newline>    arc = aredis.StrictRedisCluster(<newline>        host=host,<newline>        port=port,<newline>        password=password,<newline>        max_connections=2**31,<newline>        max_connections_per_node=2**31,<newline>        readonly=False,<newline>        reinitialize_steps=count,<newline>        skip_full_coverage_check=True,<newline>        decode_responses=False,<newline>        max_idle_time=count,<newline>        idle_check_interval=count,<newline>    )<newline>    print(f""{loop} {await warmup(arc)} aredis"")<newline>    print(await run(arc))<newline>    arc.connection_pool.disconnect()"
103	adjudicated	4	def bootstrap(tmpdir):<newline>    monkeypatch_for_cert(tmpdir)
451	adjudicated	4	def noop_function():<newline>    pass
429	adjudicated	4	class CompletionRefresher:<newline>    refreshers = OrderedDict()
243	adjudicated	4	class Context:<newline>    benchmarks: ClassVar[List[BaseRunner]] = []<newline>    stack: ExitStack = field(default_factory=ExitStack)<newline>    runner: pyperf.Runner = field(default_factory=pyperf.Runner)
344	adjudicated	4	def to_tuple(version):<newline>    return tuple(to_int(x) for x in version.split('.'))
229	adjudicated	4	"class Streaming(BaseActivity):<newline>    """"""A slimmed down version of :class:`Activity` that represents a Discord streaming status.<newline>    """""""
257	adjudicated	4	def clean_template_output(output):<newline>    output = '\n'.join(line.strip() for line in output.strip().splitlines())<newline>    output = re.sub('\n{3,}', '\n\n', output)<newline>    return output
418	adjudicated	5	"def keyring_set_password(key, passwd):<newline>    try:<newline>        keyring.set_password(""pgcli"", key, passwd)<newline>    except Exception as e:<newline>        click.secho(<newline>            keyring_error_message.format(""Set password in keyring returned:"", str(e)),<newline>            err=True,<newline>            fg=""red"",<newline>        )"
317	adjudicated	3	"def faker(request):<newline>    """"""Fixture that returns a seeded and suitable ``Faker`` instance.""""""<newline>    if ""faker_locale"" in request.fixturenames:<newline>        locale = request.getfixturevalue(""faker_locale"")<newline>        fake = Faker(locale=locale)<newline>    else:<newline>        fake = request.getfixturevalue(""_session_faker"")"
15	adjudicated	3	"def parse_line(line):<newline>  # TODO(jakevdp): should we parse other report types?<newline>  parsed = json.loads(line)<newline>  if parsed.get(""$report_type"") == ""TestReport"":<newline>    return TestReport._from_json(parsed)<newline>  return DefaultReport()"
169	adjudicated	5	"def __init__(self, environment):<newline>    """"""Initialize the extension with the given environment.""""""<newline>    super().__init__(environment)"
55	adjudicated	4	class DpEntry(NamedTuple):<newline>    position_end: int<newline>    entity_count: int<newline>    entity_lengths_sum: int<newline>    last_entity: Optional[Entity]
76	adjudicated	4	class CacheData:<newline>    def __init__(<newline>        self,<newline>        filename: str,<newline>        data_json: JsonDict,<newline>        meta_json: JsonDict,<newline>        data_size: int,<newline>        meta_size: int,<newline>    ) -> None:<newline>        self.filename = filename<newline>        self.data = data_json<newline>        self.meta = meta_json<newline>        self.data_size = data_size<newline>        self.meta_size = meta_size
68	adjudicated	4	def get_flair_corpus(data_args):<newline>    ner_task_mapping = {}
163	adjudicated	3	def runtests():<newline>    args, rest = parse_args()
439	adjudicated	5	"class CircleCIJob:<newline>    name: str<newline>    additional_env: Dict[str, Any] = None<newline>    cache_name: str = None<newline>    cache_version: str = ""0.6""<newline>    docker_image: List[Dict[str, str]] = None<newline>    install_steps: List[str] = None<newline>    marker: Optional[str] = None<newline>    parallelism: Optional[int] = 1<newline>    pytest_num_workers: int = 8<newline>    pytest_options: Dict[str, Any] = None<newline>    resource_class: Optional[str] = ""xlarge""<newline>    tests_to_run: Optional[List[str]] = None<newline>    working_directory: str = ""~/transformers"""
260	adjudicated	4	"def get_macho(fn):<newline>  # mod to make the header okay<newline>  # MH_CIGAM_64 is good<newline>  dat = open(fn, ""rb"").read()<newline>  dat = b""\xcf\xfa\xed\xfe""+dat[4:]<newline>  from tempfile import NamedTemporaryFile<newline>  with NamedTemporaryFile(delete=False) as f:<newline>    f.write(dat)<newline>    f.close()<newline>  return MachO.MachO(f.name)"
318	adjudicated	4	"def timer(func):<newline>    @functools.wraps(func)<newline>    async def wrapper(*args, **kwargs):<newline>        tic = time.perf_counter()<newline>        await func(*args, **kwargs)<newline>        toc = time.perf_counter()<newline>        return f""{toc - tic:.4f}"""
340	adjudicated	4	def set_str(conn, num, pipeline_size, data_size):<newline>    if pipeline_size > 1:<newline>        conn = conn.pipeline()
188	adjudicated	4	def __init__(self, *a, **kw):<newline>    super().__init__(*a, **kw)<newline>    if self.qps is not None:<newline>        self.qps = float(self.qps)<newline>        self.download_delay = 1 / self.qps<newline>    elif self.download_delay is not None:<newline>        self.download_delay = float(self.download_delay)
124	adjudicated	3	"def read_unformated(feats, tmp_dir):<newline>    dataset = datasets.Dataset.from_file(<newline>        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)<newline>    )<newline>    for _ in dataset:<newline>        pass"
63	adjudicated	2	"class SentenceDataset(FlairDatapointDataset):<newline>    @deprecated(version=""0.11"", reason=""The 'SentenceDataset' class was renamed to 'FlairDatapointDataset'"")<newline>    def __init__(self, sentences: Union[Sentence, List[Sentence]]):<newline>        super().__init__(sentences)"
198	adjudicated	4	def render(self, request):<newline>    now = time()<newline>    delta = now - self.lasttime
154	adjudicated	4	"class EdgeList(ColorizedList):<newline>    ARROW = SymbolWidget.SYMBOLS[""rightarrow""]"
350	adjudicated	5	"class BaseAdapter:<newline>    """"""The Base Transport Adapter"""""""
0	adjudicated	5	"def benchmark(f: Callable[[], Any], iters: Optional[int] = None,<newline>              warmup: Optional[int] = None, name: Optional[str] = None,<newline>              target_total_secs: Optional[Union[int, float]] = None):<newline>  """"""Benchmarks ``f``. Prints the results and returns the raw times.<newline>  """""""
220	adjudicated	4	"class VersionInfo(NamedTuple):<newline>    major: int<newline>    minor: int<newline>    micro: int<newline>    releaselevel: Literal[""alpha"", ""beta"", ""candidate"", ""final""]<newline>    serial: int"
472	adjudicated	4	def main():<newline>    args = parser.parse_args()<newline>    cmd, cmd_args = cmd_from_args(args)
367	adjudicated	5	def __init__(self):<newline>    self._completer_thread = None<newline>    self._restart_refresh = threading.Event()
356	adjudicated	4	"def post(url, data=None, json=None, **kwargs):<newline>    r""""""Sends a POST request.<newline>    """""""
121	adjudicated	4	"def generate_examples(features: dict, num_examples=100, seq_shapes=None):<newline>    dummy_data = []<newline>    seq_shapes = seq_shapes or {}<newline>    for i in range(num_examples):<newline>        example = {}<newline>        for col_id, (k, v) in enumerate(features.items()):<newline>            if isinstance(v, _ArrayXD):<newline>                data = np.random.rand(*v.shape).astype(v.dtype)<newline>            elif isinstance(v, datasets.Value):<newline>                if v.dtype == ""string"":<newline>                    data = ""The small grey turtle was surprisingly fast when challenged.""<newline>                else:<newline>                    data = np.random.randint(10, size=1).astype(v.dtype).item()<newline>            elif isinstance(v, datasets.Sequence):<newline>                while isinstance(v, datasets.Sequence):<newline>                    v = v.feature<newline>                shape = seq_shapes[k]<newline>                data = np.random.rand(*shape).astype(v.dtype)<newline>            example[k] = data"
290	adjudicated	5	"class Error(jose.JSONObjectWithFields, errors.Error):<newline>    """"""ACME error."""""""
415	adjudicated	5	"def _(event):<newline>    """"""Toggle between Vi and Emacs mode.""""""<newline>    _logger.debug(""Detected F5 key."")<newline>    pgcli.explain_mode = not pgcli.explain_mode"
201	adjudicated	4	def test_median_filter(shape):<newline>    x = torch.randn(*shape)
386	adjudicated	4	"def create_toolbar_tokens_func(mycli, show_fish_help):<newline>    """"""Return a function that generates the toolbar tokens.""""""<newline>    def get_toolbar_tokens():<newline>        result = []<newline>        result.append(('class:bottom-toolbar', ' '))"
347	adjudicated	5	"def test_notebooks():<newline>    for notebook in glob(""*.ipynb""):<newline>        if "" "" in notebook:<newline>            continue<newline>        print(""Testing {}"".format(notebook))<newline>        nb, errors = _notebook_run(notebook)<newline>        assert errors == []"
118	adjudicated	4	"def benchmark_iterating():<newline>    times = {""num examples"": SPEED_TEST_N_EXAMPLES}<newline>    functions = [<newline>        (read, {""length"": SMALL_TEST}),<newline>        (read, {""length"": SPEED_TEST_N_EXAMPLES}),<newline>        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 10}),<newline>        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 100}),<newline>        (read_batch, {""length"": SPEED_TEST_N_EXAMPLES, ""batch_size"": 1_000}),<newline>        (read_formatted, {""type"": ""numpy"", ""length"": SMALL_TEST}),<newline>        (read_formatted, {""type"": ""pandas"", ""length"": SMALL_TEST}),<newline>        (read_formatted, {""type"": ""torch"", ""length"": SMALL_TEST}),<newline>        (read_formatted, {""type"": ""tensorflow"", ""length"": SMALL_TEST}),<newline>        (read_formatted_batch, {""type"": ""numpy"", ""length"": SMALL_TEST, ""batch_size"": 10}),<newline>        (read_formatted_batch, {""type"": ""numpy"", ""length"": SMALL_TEST, ""batch_size"": 1_000}),<newline>    ]"
111	adjudicated	4	def __iter__(self):<newline>    return iter(self._sampled_values)
128	adjudicated	3	def select(dataset: datasets.Dataset):<newline>    _ = dataset.select(range(0, len(dataset), 2))
231	adjudicated	5	"class Cooldown:<newline>    """"""Represents a cooldown for a command.<newline>    """""""
406	adjudicated	5	"def add_subparser(self,<newline>                  parser: argparse._SubParsersAction) -> argparse.ArgumentParser:<newline>    description = """"""Cache remote files to the AllenNLP cache.""""""<newline>    subparser = parser.add_parser(<newline>        self.name,<newline>        description=description,<newline>        help=description,<newline>    )<newline>    subparser.set_defaults(func=_cached_path)<newline>    subparser.add_argument(<newline>        ""resources"",<newline>        type=str,<newline>        help=""""""The URLs or paths to the resources.<newline>        If using the --inspect or --remove flag, this can also contain glob patterns."""""",<newline>        nargs=""*"",<newline>    )<newline>    subparser.add_argument(<newline>        ""-d"",<newline>        ""--cache-dir"",<newline>        type=str,<newline>        help=""""""Use a custom cache directory."""""",<newline>        default=CACHE_DIRECTORY,<newline>    )<newline>    subparser.add_argument(<newline>        ""-x"",<newline>        ""--extract-archive"",<newline>        action=""store_true"",<newline>        help=""""""Automatically extract zip or tar.gz archive files."""""",<newline>    )<newline>    subparser.add_argument(<newline>        ""-f"",<newline>        ""--force-extract"",<newline>        action=""store_true"",<newline>        help=""""""Extract archives regardless of whether or not they already exist."""""",<newline>    )<newline>    subparser.add_argument(<newline>        ""--inspect"",<newline>        action=""store_true"",<newline>        help=""""""Print some useful information about the cache."""""",<newline>    )<newline>    subparser.add_argument(<newline>        ""--remove"",<newline>        action=""store_true"",<newline>        help=""""""Remove any cache entries matching the given resource patterns."""""",<newline>    )<newline>    return subparser"
412	adjudicated	5	"def _(event):<newline>    """"""Enable/Disable SmartCompletion Mode.""""""<newline>    _logger.debug(""Detected F2 key."")<newline>    pgcli.completer.smart_completion = not pgcli.completer.smart_completion"
65	adjudicated	4	"class TrainingArguments:<newline>    num_epochs: int = field(default=10, metadata={""help"": ""The number of training epochs.""})<newline>    batch_size: int = field(default=8, metadata={""help"": ""Batch size used for training.""})<newline>    mini_batch_chunk_size: int = field(<newline>        default=1,<newline>        metadata={""help"": ""If smaller than batch size, batches will be chunked.""},<newline>    )<newline>    learning_rate: float = field(default=5e-05, metadata={""help"": ""Learning rate""})<newline>    seed: int = field(default=42, metadata={""help"": ""Seed used for reproducible fine-tuning results.""})<newline>    device: str = field(default=""cuda:0"", metadata={""help"": ""CUDA device string.""})<newline>    weight_decay: float = field(default=0.0, metadata={""help"": ""Weight decay for optimizer.""})<newline>    embeddings_storage_mode: str = field(default=""none"", metadata={""help"": ""Defines embedding storage method.""})"
178	adjudicated	4	"def load_response(url: str, filename: str) -> HtmlResponse:<newline>    input_path = Path(__file__).parent / ""_tests"" / filename<newline>    return HtmlResponse(url, body=input_path.read_bytes())"
126	adjudicated	3	"def read_batch_unformated(feats, tmp_dir):<newline>    batch_size = 10<newline>    dataset = datasets.Dataset.from_file(<newline>        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)<newline>    )<newline>    for i in range(0, len(dataset), batch_size):<newline>        _ = dataset[i : i + batch_size]"
474	adjudicated	5	"def checkpoint_metric(checkpoint_path):<newline>    if not checkpoint_path or not os.path.isfile(checkpoint_path):<newline>        return {}<newline>    print(""=> Extracting metric from checkpoint '{}'"".format(checkpoint_path))<newline>    checkpoint = torch.load(checkpoint_path, map_location='cpu')<newline>    metric = None<newline>    if 'metric' in checkpoint:<newline>        metric = checkpoint['metric']<newline>    elif 'metrics' in checkpoint and 'metric_name' in checkpoint:<newline>        metrics = checkpoint['metrics']<newline>        print(metrics)<newline>        metric = metrics[checkpoint['metric_name']]<newline>    return metric"
132	adjudicated	3	def shard(dataset: datasets.Dataset, num_shards=10):<newline>    for shard_id in range(num_shards):<newline>        _ = dataset.shard(num_shards, shard_id)
409	adjudicated	5	"def _dummy_output(args: argparse.Namespace):<newline>   logger.info(<newline>        ""The checklist integration of allennlp is optional; if you're using conda, ""<newline>        ""it can be installed with `conda install allennlp-checklist`, ""<newline>        ""otherwise use `pip install allennlp[checklist]`.""<newline>    )"
77	adjudicated	4	def total_size(self) -> int:<newline>    return self.data_size + self.meta_size
172	adjudicated	5	"def get_config(config_path):<newline>    """"""Retrieve the config from the specified path, returning a config dict.""""""<newline>    if not os.path.exists(config_path):<newline>        raise ConfigDoesNotExistException(f'Config file {config_path} does not exist.')"
13	adjudicated	4	def create_cluster(region):<newline>    logging.debug(requests.post(_API_URL, json={'name':'create_cluster', 'region':region},<newline>                                auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS))
164	adjudicated	4	"def pytest_addoption(parser):<newline>    parser.addoption(<newline>        ""--deprecation"",<newline>        choices=[""all"", ""pending"", ""imminent"", ""none""],<newline>        default=""pending"",<newline>    )<newline>    parser.addoption(""--postgres"", action=""store_true"")<newline>    parser.addoption(""--elasticsearch"", action=""store_true"")"
48	adjudicated	3	class BitDiffusion(DiffusionPipeline):<newline>    def __init__(<newline>        self,<newline>        unet: UNet2DConditionModel,<newline>        scheduler: Union[DDIMScheduler, DDPMScheduler],<newline>        bit_scale: Optional[float] = 1.0,<newline>    ):<newline>        super().__init__()<newline>        self.bit_scale = bit_scale<newline>        self.scheduler.step = (<newline>            ddim_bit_scheduler_step if isinstance(scheduler, DDIMScheduler) else ddpm_bit_scheduler_step<newline>        )
171	adjudicated	5	"def merge_configs(default, overwrite):<newline>    """"""Recursively update a dict with the key/value pair of another.<newline><newline>    def get_config(config_path):<newline>    """"""Retrieve the config from the specified path, returning a config dict.""""""<newline>    if not os.path.exists(config_path):<newline>        raise ConfigDoesNotExistException(f'Config file {config_path} does not exist.')"
262	adjudicated	4	def fj(x):<newline>  ss = []<newline>  for i in range(0, 0x10, 4):<newline>    ss.append(' '.join(x[i:i+4]))<newline>  return '  '.join(ss)
50	adjudicated	5	"def bits_to_decimal(x, bits=BITS):<newline>    """"""expects bits from -1 to 1, outputs image tensor from 0 to 1""""""<newline>    device = x.device"
104	adjudicated	4	def map(dataset: datasets.Dataset, **kwargs):<newline>    _ = dataset.map(**kwargs)
487	adjudicated	4	def _tensordot(a, b, axes=2):<newline>    # Much of this is stolen from numpy/core/numeric.py::tensordot<newline>    # Please see license at https://github.com/numpy/numpy/blob/master/LICENSE.txt<newline>    try:<newline>        iter(axes)<newline>    except TypeError:<newline>        axes_a = list(range(-axes, 0))<newline>        axes_b = list(range(0, axes))<newline>    else:<newline>        axes_a, axes_b = axes<newline>    try:<newline>        na = len(axes_a)<newline>        axes_a = list(axes_a)<newline>    except TypeError:<newline>        axes_a = [axes_a]<newline>        na = 1<newline>    try:<newline>        nb = len(axes_b)<newline>        axes_b = list(axes_b)<newline>    except TypeError:<newline>        axes_b = [axes_b]<newline>        nb = 1
313	adjudicated	4	def print_provider(<newline>    doc: Documentor,<newline>    provider: BaseProvider,<newline>    formatters: Dict[str, T],<newline>    excludes: Optional[List[str]] = None,<newline>    output: Optional[TextIO] = None,<newline>) -> None:<newline>    if output is None:<newline>        output = sys.stdout<newline>    if excludes is None:<newline>        excludes = []
119	adjudicated	4	def get_duration(func):<newline>    def wrapper(*args, **kwargs):<newline>        starttime = timeit.default_timer()<newline>        _ = func(*args, **kwargs)<newline>        delta = timeit.default_timer() - starttime<newline>        return delta
319	adjudicated	4	"async def wrapper(*args, **kwargs):<newline>    tic = time.perf_counter()<newline>    await func(*args, **kwargs)<newline>    toc = time.perf_counter()<newline>    return f""{toc - tic:.4f}"""
434	adjudicated	4	class CustomOutputChecker(OutputChecker):<newline>    def check_output(self, want, got, optionflags):<newline>        if IGNORE_RESULT & optionflags:<newline>            return True<newline>        return OutputChecker.check_output(self, want, got, optionflags)
199	adjudicated	4	def test_dtw(N: int, M: int):<newline>    steps = np.concatenate([np.zeros(N - 1), np.ones(M - 1)])<newline>    np.random.shuffle(steps)<newline>    x = np.random.random((N, M)).astype(np.float32)
329	adjudicated	5	"class ListJoiningConnection(Connection):<newline>    def send_packed_command(self, command, check_health=True):<newline>        if not self._sock:<newline>            self.connect()<newline>        try:<newline>            if isinstance(command, str):<newline>                command = [command]<newline>            for item in command:<newline>                self._sock.sendall(item)<newline>        except OSError as e:<newline>            self.disconnect()<newline>            if len(e.args) == 1:<newline>                _errno, errmsg = ""UNKNOWN"", e.args[0]<newline>            else:<newline>                _errno, errmsg = e.args<newline>            raise ConnectionError(f""Error {_errno} while writing to socket. {errmsg}."")<newline>        except Exception:<newline>            self.disconnect()<newline>            raise"
41	adjudicated	4	def __init__(<newline>    self,<newline>    vae: AutoencoderKL,<newline>    text_encoder: CLIPTextModel,<newline>    tokenizer: CLIPTokenizer,<newline>    unet: UNet2DConditionModel,<newline>    scheduler: Union[<newline>        DDIMScheduler,<newline>        PNDMScheduler,<newline>        LMSDiscreteScheduler,<newline>        EulerDiscreteScheduler,<newline>        EulerAncestralDiscreteScheduler,<newline>        DPMSolverMultistepScheduler,<newline>    ],<newline>    safety_checker: StableDiffusionSafetyChecker,<newline>    feature_extractor: CLIPFeatureExtractor,<newline>    requires_safety_checker: bool = True,<newline>):<newline>    super().__init__()
71	adjudicated	4	"class DataPoint:<newline>    """"""<newline>    This is the parent class of all data points in Flair (including Token, Sentence, Image, etc.). Each DataPoint<newline>    must be embeddable (hence the abstract property embedding() and methods to() and clear_embeddings()). Also,<newline>    each DataPoint may have Labels in several layers of annotation (hence the functions add_label(), get_labels()<newline>    and the property 'label')<newline>    """""""
490	adjudicated	5	"def coarsen(reduction, x, axes, trim_excess=False, **kwargs):<newline>    """"""Coarsen array by applying reduction to fixed size neighborhoods<newline>    """""""
184	adjudicated	4	"def setup(app):<newline>    app.connect(""autodoc-skip-member"", maybe_skip_member)"
56	adjudicated	5	"class ClassificationCorpus(Corpus):<newline>    """"""<newline>    A classification corpus from FastText-formatted text files.<newline>    """""""
186	adjudicated	5	def main():<newline>    # Used for remembering the file (and its contents)<newline>    # so we don't have to open the same file again.<newline>    _filename = None<newline>    _contents = None
258	adjudicated	4	def load_database() -> Database:<newline>    return yaml.safe_load(DB_FILE.read_text(encoding='utf-8'))
426	adjudicated	4	def format_output(self, cur, headers, **output_kwargs):<newline>    # explain query results should always contain 1 row each<newline>    [(data,)] = list(cur)<newline>    explain_list = json.loads(data)<newline>    visualizer = Visualizer(self.max_width)<newline>    for explain in explain_list:<newline>        visualizer.load(explain)<newline>        yield visualizer.get_list()
241	adjudicated	4	"def discover():<newline>    res = {}<newline>    try:<newline>        for k in Cache:<newline>            _type = Cache[k]<newline>            rec = {""host"": k[0], ""port"": k[1]}<newline>            if _type not in res:<newline>                res[_type] = [rec]<newline>            else:<newline>                res[_type].append(rec)<newline>    except Exception as e:<newline>        return {""error"": str(e)}, 500<newline>    return res, 200"
278	adjudicated	4	def encode(self, value: Any) -> Any:<newline>    if value != self.value:<newline>        logger.warning(<newline>            'Overriding fixed field (%s) with %r', self.json_name, value)<newline>    return value
224	adjudicated	4	"class Parameter:<newline>    """"""A class that contains the parameter information of a :class:`Command` callback.<newline>    """""""
62	adjudicated	3	"class FlairDatapointDataset(FlairDataset, Generic[DT]):<newline>    """"""<newline>    A simple Dataset object to wrap a List of Datapoints, for example Sentences<newline>    """""""
156	adjudicated	4	class ChartResultsView:<newline>    def __init__(self, parent, chart, grammar, toplevel=True):<newline>        self._chart = chart<newline>        self._grammar = grammar<newline>        self._trees = []<newline>        self._y = 10<newline>        self._treewidgets = []<newline>        self._selection = None<newline>        self._selectbox = None
123	adjudicated	3	"def write(my_features, dummy_data, tmp_dir):<newline>    with ArrowWriter(features=my_features, path=os.path.join(tmp_dir, ""beta.arrow"")) as writer:<newline>        for key, record in dummy_data:<newline>            example = my_features.encode_example(record)<newline>            writer.write(example)<newline>        num_examples, num_bytes = writer.finalize()"
18	adjudicated	5	"def required_devices(num_devices_required):<newline>  """"""Helper to skip benchmarks that require more devices.""""""<newline>  def helper1(f):<newline>    @functools.wraps(f)<newline>    def helper2(state):<newline>      if jax.device_count() < num_devices_required:<newline>        state.skip_with_error(f""requires {num_devices_required} devices"")<newline>        return<newline>      return f(state)<newline>    return helper2<newline>  return helper1"
353	adjudicated	5	"def get(url, params=None, **kwargs):<newline>    r""""""Sends a GET request.<newline>    """"""<newline>"
78	adjudicated	4	def extract_classes(chunks: Iterable[CacheData]) -> Iterable[JsonDict]:<newline>    def extract(chunks: Iterable[JsonDict]) -> Iterable[JsonDict]:<newline>        for chunk in chunks:<newline>            if isinstance(chunk, dict):<newline>                yield chunk<newline>                yield from extract(chunk.values())<newline>            elif isinstance(chunk, list):<newline>                yield from extract(chunk)
448	adjudicated	4	"def main():<newline>    # Create deployment<newline>    if Version(prefect.__version__) < Version(""2.1.0""):<newline>        deployment = Deployment(<newline>            name=""test-deployment"",<newline>            flow_name=hello.name,<newline>            parameter_openapi_schema=parameter_schema(hello),<newline>            path=str(pathlib.Path(__file__).parent),<newline>            entrypoint=f""{__file__}:hello"",<newline>        )<newline>        deployment_id = anyio.run(apply_deployment_20, deployment)<newline>    else:<newline>        deployment = Deployment.build_from_flow(flow=hello, name=""test-deployment"")<newline>        deployment_id = deployment.apply()"
81	adjudicated	4	"class Aw(Awaitable[int]):<newline>    def __await__(self) -> Generator[str, Any, int]:<newline>        yield ""a""<newline>        return 1"
245	adjudicated	5	"class CommandRunner(BaseRunner):<newline>    """"""<newline>    Run a single command, and benchmark it.<newline>    """""""
122	adjudicated	3	def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):<newline>    dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)
82	adjudicated	4	"def plain_generator() -> Generator[str, None, int]:<newline>    yield ""a""<newline>    return 1"
427	adjudicated	4	"def load_ipython_extension(ipython):<newline>    """"""This is called via the ipython command '%load_ext pgcli.magic'"""""""
273	adjudicated	5	"class _TokenChallenge(Challenge):<newline>    """"""Challenge with token."""""""
333	adjudicated	4	"async def warmup(client):<newline>    await asyncio.gather(<newline>        *(asyncio.create_task(client.exists(f""bench:warmup_{i}"")) for i in range(100))<newline>    )"
32	adjudicated	4	def preprocess(image):<newline>    if isinstance(image, torch.Tensor):<newline>        return image<newline>    elif isinstance(image, PIL.Image.Image):<newline>        image = [image]
365	adjudicated	4	"def _check_cryptography(cryptography_version):<newline>    # cryptography < 1.3.4<newline>    try:<newline>        cryptography_version = list(map(int, cryptography_version.split(""."")))<newline>    except ValueError:<newline>        return"
130	adjudicated	3	def shuffle(dataset: datasets.Dataset):<newline>    _ = dataset.shuffle()
1	adjudicated	5	"def benchmark_suite(prepare: Callable[..., Callable], params_list: List[Dict],<newline>                    name: str, target_total_secs: Optional[int] = None):<newline>  """"""Benchmarks a function for several combinations of parameters.<newline>  """""""
207	adjudicated	4	"def pytest_configure(config):<newline>    config.addinivalue_line(""markers"", ""requires_cuda"")"
173	adjudicated	5	"class ConfigDoesNotExistException(CookiecutterException):<newline>    """"""<newline>    Exception for missing config file.<newline>    """""""
399	adjudicated	5	def run():<newline>    from allennlp.commands import main  # noqa<newline>    from allennlp.common.util import install_sigterm_handler
477	adjudicated	4	"def main():<newline>    parser = argparse.ArgumentParser(description=""Model index generation config"")<newline>    parser.add_argument(<newline>        ""-t"",<newline>        ""--templates"",<newline>        default=Path(__file__).parent / "".templates"",<newline>        type=str,<newline>        help=""Location of the markdown templates"",<newline>    )<newline>    parser.add_argument(<newline>        ""-d"",<newline>        ""--dest"",<newline>        default=Path(__file__).parent / ""models"",<newline>        type=str,<newline>        help=""Destination folder that contains the generated model-index files."",<newline>    )<newline>    args = parser.parse_args()<newline>    templates_path = Path(args.templates)<newline>    dest_readmes_path = Path(args.dest)"
284	adjudicated	5	"class SchemaValidationError(jose_errors.DeserializationError):<newline>    """"""JSON schema ACME object validation error."""""""
19	adjudicated	5	"def create_mesh(shape, axis_names, state):<newline>  size = np.prod(shape)<newline>  if len(jax.devices()) < size:<newline>    state.skip_with_error(f""Requires {size} devices"")<newline>    return None<newline>  devices = sorted(jax.devices(), key=lambda d: d.id)<newline>  mesh_devices = np.array(devices[:size]).reshape(shape)<newline>  global_mesh = jax.sharding.Mesh(mesh_devices, axis_names)<newline>  return global_mesh"
148	adjudicated	4	"def __init__(self, image, initialField, initialText):<newline>    frm = Frame(root)<newline>    frm.config(background=""white"")<newline>    self.image = PhotoImage(format=""gif"", data=images[image.upper()])<newline>    self.imageDimmed = PhotoImage(format=""gif"", data=images[image])<newline>    self.img = Label(frm)<newline>    self.img.config(borderwidth=0)<newline>    self.img.pack(side=""left"")<newline>    self.fld = Text(frm, **fieldParams)<newline>    self.initScrollText(frm, self.fld, initialField)<newline>    frm = Frame(root)<newline>    self.txt = Text(frm, **textParams)<newline>    self.initScrollText(frm, self.txt, initialText)<newline>    for i in range(2):<newline>        self.txt.tag_config(colors[i], background=colors[i])<newline>        self.txt.tag_config(""emph"" + colors[i], foreground=emphColors[i])"
259	adjudicated	5	"def benchmark(ane):<newline>  tin = ANETensor(512*0x20)<newline>  tout = ANETensor(512*0x20)<newline>  dat = open(""../ops/gemm.hwx"", ""rb"").read()<newline>  for k,v in ane.debug(dat[0x4000:0x4300], 16).items():<newline>    print(k,v)<newline>  comp = ane.compile(dat)"
67	adjudicated	3	"class DataArguments:<newline>    dataset_name: str = field(metadata={""help"": ""Flair NER dataset name.""})<newline>    dataset_arguments: str = field(default="""", metadata={""help"": ""Dataset arguments for Flair NER dataset.""})<newline>    output_dir: str = field(<newline>        default=""resources/taggers/ner"",<newline>        metadata={""help"": ""Defines output directory for final fine-tuned model.""},<newline>    )"
375	adjudicated	5	"def _(event):<newline>    """"""Enable/Disable Multiline Mode.""""""<newline>    _logger.debug('Detected F3 key.')<newline>    mycli.multi_line = not mycli.multi_line"
140	adjudicated	4	"def _init_bindings(self):<newline>    # Key bindings are a good thing.<newline>    self._top.bind(""<Control-q>"", self.destroy)<newline>    self._top.bind(""<Control-x>"", self.destroy)<newline>    self._top.bind(""<Escape>"", self.destroy)<newline>    self._top.bind(""e"", self.expand)<newline>    # self._top.bind('<Alt-e>', self.expand)<newline>    # self._top.bind('<Control-e>', self.expand)<newline>    self._top.bind(""m"", self.match)<newline>    self._top.bind(""<Alt-m>"", self.match)<newline>    self._top.bind(""<Control-m>"", self.match)<newline>    self._top.bind(""b"", self.backtrack)<newline>    self._top.bind(""<Alt-b>"", self.backtrack)<newline>    self._top.bind(""<Control-b>"", self.backtrack)<newline>    self._top.bind(""<Control-z>"", self.backtrack)<newline>    self._top.bind(""<BackSpace>"", self.backtrack)<newline>    self._top.bind(""a"", self.autostep)<newline>    # self._top.bind('<Control-a>', self.autostep)<newline>    self._top.bind(""<Control-space>"", self.autostep)<newline>    self._top.bind(""<Control-c>"", self.cancel_autostep)<newline>    self._top.bind(""<space>"", self.step)<newline>    self._top.bind(""<Delete>"", self.reset)<newline>    self._top.bind(""<Control-p>"", self.postscript)<newline>    # self._top.bind('<h>', self.help)<newline>    # self._top.bind('<Alt-h>', self.help)<newline>    self._top.bind(""<Control-h>"", self.help)<newline>    self._top.bind(""<F1>"", self.help)<newline>    # self._top.bind('<g>', self.toggle_grammar)<newline>    # self._top.bind('<Alt-g>', self.toggle_grammar)<newline>    # self._top.bind('<Control-g>', self.toggle_grammar)<newline>    self._top.bind(""<Control-g>"", self.edit_grammar)<newline>    self._top.bind(""<Control-t>"", self.edit_sentence)"
170	adjudicated	5	"def _expand_path(path):<newline>    """"""Expand both environment variables and user home in the given path.""""""<newline>    path = os.path.expandvars(path)<newline>    path = os.path.expanduser(path)<newline>    return path"
117	adjudicated	4	def read_formatted_batch(dataset: datasets.Dataset, length, batch_size, type):<newline>    with dataset.formatted_as(type=type):<newline>        for i in range(0, length, batch_size):<newline>            _ = dataset[i : i + batch_size]
359	adjudicated	5	"class HTTPBasicAuth(AuthBase):<newline>    """"""Attaches HTTP Basic Authentication to the given Request object."""""""
384	adjudicated	4	def cond():<newline>    doc = get_app().layout.get_buffer_by_name(DEFAULT_BUFFER).document
11	adjudicated	4	def get_cluster_ip(region):<newline>    return requests.post(_API_URL, json={'name':'get_cluster_ip', 'region':region},<newline>                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['cluster_ip']
296	adjudicated	5	def sign(cls, payload: bytes, key: jose.JWK, alg: jose.JWASignature, nonce: Optional[bytes],<newline>         url: Optional[str] = None, kid: Optional[str] = None) -> jose.JWS:<newline>    # Per ACME spec, jwk and kid are mutually exclusive, so only include a<newline>    # jwk field if kid is not provided.<newline>    include_jwk = kid is None<newline>    return super().sign(payload, key=key, alg=alg,<newline>                        protect=frozenset(['nonce', 'url', 'kid', 'jwk', 'alg']),<newline>                        nonce=nonce, url=url, kid=kid,<newline>                        include_jwk=include_jwk)
326	adjudicated	4	def setup(self, **kwargs):<newline>    pass
138	adjudicated	4	def __init__(self, grammar, sent, trace=0):<newline>    self._sent = sent<newline>    self._parser = SteppingRecursiveDescentParser(grammar, trace)
28	adjudicated	4	"def main(logfile: str, outmd: str, outjson: str, name: str):<newline>    print(f""Extracting content of {logfile}"")<newline>    print(f""and writing to {outmd} and {outjson}"")"
244	adjudicated	5	"class BaseRunner:<newline>    """"""<newline>    An individual benchmark case. By default it has the category<newline>    (e.g like startup or download) and a name.<newline>    """""""
16	adjudicated	4	"def main(logfile, outfile):<newline>  logging.info(""Parsing %s"", logfile)<newline>  try:<newline>    with open(logfile, 'r') as f:<newline>      reports = (parse_line(line) for line in f)<newline>      failures = (r for r in reports if r.outcome == ""failed"")<newline>      summary = ""\n"".join(f""{f.nodeid}: {f.longrepr.chain[0][1].message}""<newline>                          for f in failures)<newline>    logging.info(""Parsed summary:\n%s"", summary)<newline>  except Exception:<newline>    err_info = traceback.format_exc()<newline>    logging.info(""Parsing failed:\n%s"", err_info)<newline>    summary = f""Log parsing failed; traceback:\n\n{err_info}""<newline>  logging.info(""Writing result to %s"", outfile)<newline>  with open(outfile, 'w') as f:<newline>    f.write(MSG_FORMAT.format(summary=summary))"
459	adjudicated	4	def bench_flow_decorator(benchmark: BenchmarkFixture):<newline>    benchmark(flow, noop_function)
280	adjudicated	5	"class SSLSocket:  # pylint: disable=too-few-public-methods<newline>    """"""SSL wrapper for sockets."""""""
346	adjudicated	4	"def _notebook_run(path):<newline>    """"""Execute a notebook via nbconvert and collect output.<newline>       :returns (parsed nb object, execution errors)<newline>    """"""<newline>    kernel_name = 'python%d' % sys.version_info[0]<newline>    this_file_directory = os.path.dirname(__file__)<newline>    errors = []<newline>    with tempfile.NamedTemporaryFile(suffix="".ipynb"", mode='wt') as fout:<newline>        with smart_open(path, 'rb') as f:<newline>            nb = nbformat.read(f, as_version=4)<newline>            nb.metadata.get('kernelspec', {})['name'] = kernel_name<newline>            ep = ExecutePreprocessor(kernel_name=kernel_name, timeout=10)"
372	adjudicated	5	"def parse_pygments_style(token_name, style_object, style_dict):<newline>    """"""Parse token type and style string.<newline>    """""""
282	adjudicated	5	"class Error(Exception):<newline>    """"""Generic ACME error."""""""
463	adjudicated	4	def map_mx_to_torch_model(mx_name):<newline>    torch_name = mx_name.lower()<newline>    if torch_name.startswith('se_'):<newline>        torch_name = torch_name.replace('se_', 'se')<newline>    elif torch_name.startswith('senet_'):<newline>        torch_name = torch_name.replace('senet_', 'senet')<newline>    elif torch_name.startswith('inceptionv3'):<newline>        torch_name = torch_name.replace('inceptionv3', 'inception_v3')<newline>    torch_name = 'gluon_' + torch_name<newline>    return torch_name
115	adjudicated	4	def read_batch(dataset: datasets.Dataset, length, batch_size):<newline>    for i in range(0, len(dataset), batch_size):<newline>        _ = dataset[i : i + batch_size]
457	adjudicated	4	def noop_function():<newline>    pass
189	adjudicated	4	"def start_requests(self):<newline>    url = self.benchurl<newline>    if self.latency is not None:<newline>        url += f""?latency={self.latency}"""
363	adjudicated	4	"def unicode_is_ascii(u_string):<newline>    """"""Determine if unicode string only contains ASCII characters.<newline>    """""""
73	adjudicated	4	def make_cache(input_dir: str, sqlite: bool) -> MetadataStore:<newline>    if sqlite:<newline>        return SqliteMetadataStore(input_dir)<newline>    else:<newline>        return FilesystemMetadataStore(input_dir)
100	adjudicated	4	def determine_pip_install_arguments():<newline>    implicit_pip = True<newline>    implicit_setuptools = False<newline>    implicit_wheel = True
87	adjudicated	3	"def pytest_addoption(parser) -> None:<newline>    parser.addoption(<newline>        ""--bench"", action=""store_true"", default=False, help=""Enable the benchmark test runs""<newline>    )"
469	adjudicated	4	class ProfileRunner(BenchmarkRunner):
310	adjudicated	5	"class GaussianBlur(ImageOnlyTransform):<newline>    """"""Blur the input image using a Gaussian filter with a random kernel size.<newline>    """""""
35	adjudicated	5	"class CheckpointMergerPipeline(DiffusionPipeline):<newline>    """"""<newline>    A class that that supports merging diffusion models based on the discussion here:<newline>    https://github.com/huggingface/diffusers/issues/877<newline><newline>    def __init__(self):<newline>        self.register_to_config()<newline>        super().__init__()<newline><newline>    def _compare_model_configs(self, dict0, dict1):<newline>        if dict0 == dict1:<newline>            return True<newline>        else:<newline>            config0, meta_keys0 = self._remove_meta_keys(dict0)<newline>            config1, meta_keys1 = self._remove_meta_keys(dict1)<newline>            if config0 == config1:<newline>                print(f""Warning !: Mismatch in keys {meta_keys0} and {meta_keys1}."")<newline>                return True<newline>        return False<newline><newline>    def _remove_meta_keys(self, config_dict: Dict):<newline>        meta_keys = []<newline>        temp_dict = config_dict.copy()<newline>        for key in config_dict.keys():<newline>            if key.startswith(""_""):<newline>                temp_dict.pop(key)<newline>                meta_keys.append(key)<newline>        return (temp_dict, meta_keys)"
101	adjudicated	5	"def monkeypatch_for_cert(tmpdir):<newline>    """"""Patches `pip install` to provide default certificate with the lowest priority.<newline>    """""""
44	adjudicated	4	"def enable_sequential_cpu_offload(self, gpu_id=0):<newline>    r""""""<newline>    Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,<newline>    text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a<newline>    `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.<newline>    """"""<newline>    if is_accelerate_available():<newline>        from accelerate import cpu_offload<newline>    else:<newline>        raise ImportError(""Please install accelerate via `pip install accelerate`"")"
31	adjudicated	4	"class DDIMNoiseComparativeAnalysisPipeline(DiffusionPipeline):<newline>    r""""""<newline>    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the<newline>    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)<newline>    """""""
14	adjudicated	4	"class DefaultReport:<newline>  outcome : str = ""none"""
297	adjudicated	5	"class ClientV2:<newline>    """"""ACME client for a v2 API."""""""
89	adjudicated	3	"def main() -> None:<newline>    parser = argparse.ArgumentParser()<newline>    parser.add_argument(<newline>        ""--typeshed-dir"", help=""location of typeshed"", metavar=""dir"", required=True<newline>    )<newline>    parser.add_argument(""commit"", help=""typeshed commit hash to cherry-pick"")<newline>    args = parser.parse_args()<newline>    typeshed_dir = args.typeshed_dir<newline>    commit = args.commit"
331	adjudicated	4	"def timer(func):<newline>    @functools.wraps(func)<newline>    async def wrapper(*args, **kwargs):<newline>        tic = time.perf_counter()<newline>        await func(*args, **kwargs)<newline>        toc = time.perf_counter()<newline>        return f""{toc - tic:.4f}"""
330	adjudicated	4	"def pack_command(self, *args):<newline>    ""Pack a series of arguments into a value Redis command""<newline>    args_output = SYM_EMPTY.join(<newline>        [<newline>            SYM_EMPTY.join(<newline>                (SYM_DOLLAR, str(len(k)).encode(), SYM_CRLF, k, SYM_CRLF)<newline>            )<newline>            for k in map(self.encoder.encode, args)<newline>        ]<newline>    )<newline>    output = SYM_EMPTY.join(<newline>        (SYM_STAR, str(len(args)).encode(), SYM_CRLF, args_output)<newline>    )<newline>    return output"
354	adjudicated	5	"def options(url, **kwargs):<newline>    r""""""Sends an OPTIONS request.<newline>    """""""
390	adjudicated	3	def count_instances_from_args(args: argparse.Namespace):<newline>    from allennlp.training.util import data_loaders_from_params
445	adjudicated	4	async def apply_deployment_20(deployment):<newline>    async with prefect.get_client() as client:<newline>        flow_id = await client.create_flow_from_name(deployment.flow_name)<newline>        return await client.create_deployment(<newline>            flow_id=flow_id,<newline>            name=deployment.name,<newline>            path=deployment.path,<newline>            entrypoint=deployment.entrypoint,<newline>        )
134	adjudicated	5	"def normalize_grammar(self, grammar):<newline>    # Strip comments<newline>    grammar = re.sub(r""((\\.|[^#])*)(#.*)?"", r""\1"", grammar)<newline>    # Normalize whitespace<newline>    grammar = re.sub("" +"", "" "", grammar)<newline>    grammar = re.sub(r""\n\s+"", r""\n"", grammar)<newline>    grammar = grammar.strip()<newline>    # [xx] Hack: automatically backslash $!<newline>    grammar = re.sub(r""([^\\])\$"", r""\1\\$"", grammar)<newline>    return grammar"
195	adjudicated	4	def __init__(self):<newline>    Resource.__init__(self)<newline>    self.concurrent = 0<newline>    self.tail = deque(maxlen=100)<newline>    self._reset_stats()
95	adjudicated	4	def cli(<newline>    ctx,<newline>    state,<newline>    where=False,<newline>    venv=False,<newline>    py=False,<newline>    envs=False,<newline>    rm=False,<newline>    bare=False,<newline>    man=False,<newline>    support=None,<newline>    help=False,<newline>    site_packages=None,<newline>    **kwargs,<newline>):<newline>    from pipenv.patched.pip._vendor import rich<newline>    from pipenv.utils.shell import system_which
61	adjudicated	5	class DataLoader(torch.utils.data.dataloader.DataLoader):<newline>    def __init__(<newline>        self,<newline>        dataset,<newline>        batch_size=1,<newline>        shuffle=False,<newline>        sampler=None,<newline>        batch_sampler=None,<newline>        num_workers=None,<newline>        drop_last=False,<newline>        timeout=0,<newline>        worker_init_fn=None,<newline>    ):<newline>        # in certain cases, multi-CPU data loading makes no sense and slows<newline>        # everything down. For this reason, we detect if a dataset is in-memory:<newline>        # if so, num_workers is set to 0 for faster processing<newline>        flair_dataset = dataset<newline>        while True:<newline>            if type(flair_dataset) is Subset:<newline>                flair_dataset = flair_dataset.dataset<newline>            elif type(flair_dataset) is ConcatDataset:<newline>                flair_dataset = flair_dataset.datasets[0]<newline>            else:<newline>                break
425	adjudicated	4	def __init__(self, max_width):<newline>    self.max_width = max_width
232	adjudicated	4	def __init__(self, rate: float, per: float) -> None:<newline>    self.rate: int = int(rate)<newline>    self.per: float = float(per)<newline>    self._window: float = 0.0<newline>    self._tokens: int = self.rate<newline>    self._last: float = 0.0
294	adjudicated	4	"class JWS(jose.JWS):<newline>    """"""ACME-specific JWS. Includes none, url, and kid in protected header.""""""<newline>    signature_cls = Signature<newline>    __slots__ = jose.JWS._orig_slots  # type: ignore[attr-defined]  # pylint: disable=protected-access"
446	adjudicated	4	"async def create_flow_run(deployment_id):<newline>    async with prefect.get_client() as client:<newline>        return await client.create_flow_run_from_deployment(<newline>            deployment_id, parameters={""name"": ""integration tests""}<newline>        )"
137	adjudicated	5	"class RecursiveDescentApp:<newline>    """"""<newline>    A graphical tool for exploring the recursive descent parser.  The tool<newline>    displays the parser's tree and the remaining text, and allows the<newline>    user to control the parser's operation.  In particular, the user<newline>    can expand subtrees on the frontier, match tokens on the frontier<newline>    against the text, and backtrack.  A ""step"" button simply steps<newline>    through the parsing process, performing the operations that<newline>    ``RecursiveDescentParser`` would use.<newline>    """"""<newline><newline>    def __init__(self, grammar, sent, trace=0):<newline>        self._sent = sent<newline>        self._parser = SteppingRecursiveDescentParser(grammar, trace)<newline><newline>"
29	adjudicated	4	class MakeCutouts(nn.Module):<newline>  def __init__(self, cut_size, cut_power=1.0):<newline>      super().__init__()
4	adjudicated	3	def _param_str(param):<newline>  if callable(param):<newline>    return param.__name__<newline>  return str(param)
43	adjudicated	4	"def disable_vae_slicing(self):<newline>    r""""""<newline>    Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to<newline>    computing decoding in one step.<newline>    """"""<newline>    self.vae.disable_slicing()"
381	adjudicated	4	"def read_config_files(files, list_values=True):<newline>    """"""Read and merge a list of config files."""""""
108	adjudicated	4	class RandIter:<newline>    low: int<newline>    high: int<newline>    size: int<newline>    seed: int
484	adjudicated	4	class NumpyBackendEntrypoint(ArrayBackendEntrypoint):<newline>    @classmethod<newline>    def to_backend_dispatch(cls):<newline>        return to_numpy_dispatch
180	adjudicated	4	class settingslist_node(nodes.General, nodes.Element):<newline>    pass
268	adjudicated	4	"class vm_region_submap_short_info_data_64(ctypes.Structure):<newline>  _pack_ = 1<newline>  _fields_ = [<newline>      (""protection"", ctypes.c_uint32),<newline>      (""max_protection"", ctypes.c_uint32),<newline>      (""inheritance"", ctypes.c_uint32),<newline>      (""offset"", ctypes.c_ulonglong),<newline>      (""user_tag"", ctypes.c_uint32),<newline>      (""ref_count"", ctypes.c_uint32),<newline>      (""shadow_depth"", ctypes.c_uint16),<newline>      (""external_pager"", ctypes.c_byte),<newline>      (""share_mode"", ctypes.c_byte),<newline>      (""is_submap"", ctypes.c_uint32),<newline>      (""behavior"", ctypes.c_uint32),<newline>      (""object_id"", ctypes.c_uint32),<newline>      (""user_wired_count"", ctypes.c_uint32),<newline>  ]<newline>submap_info_size = ctypes.sizeof(vm_region_submap_short_info_data_64) // 4"
388	adjudicated	4	"def _get_vi_mode():<newline>    """"""Get the current vi mode for display.""""""<newline>    return {<newline>        InputMode.INSERT: 'I',<newline>        InputMode.NAVIGATION: 'N',<newline>        InputMode.REPLACE: 'R',<newline>        InputMode.REPLACE_SINGLE: 'R',<newline>        InputMode.INSERT_MULTIPLE: 'M',<newline>    }[get_app().vi_state.input_mode]"
316	adjudicated	5	"def _session_faker(request):<newline>    """"""Fixture that stores the session level ``Faker`` instance."
265	adjudicated	4	"def init_libane():<newline>  global libane, aneregs<newline>  libane = cdll.LoadLibrary(os.path.join(basedir, ""libane.dylib""))"
27	adjudicated	4	def get_benchmark_fn(nargs, nshards):<newline>  pmap_fn = pmap(lambda *args: jnp.sum(jnp.array(args)))<newline>  shape = (nshards, 4)<newline>  args = [jnp.array(np.random.random(shape)) for _ in range(nargs)]<newline>  assert all(isinstance(arg, jax.Array) for arg in args)<newline>  def benchmark_fn():<newline>    for _ in range(10):<newline>      pmap_fn(*args)<newline>  return benchmark_fn
227	adjudicated	4	"class Activity(BaseActivity):<newline>    """"""Represents an activity in Discord.<newline>    """""""
145	adjudicated	4	"class Zone:<newline>    def __init__(self, image, initialField, initialText):<newline>        frm = Frame(root)<newline>        frm.config(background=""white"")<newline>        self.image = PhotoImage(format=""gif"", data=images[image.upper()])<newline>        self.imageDimmed = PhotoImage(format=""gif"", data=images[image])<newline>        self.img = Label(frm)<newline>        self.img.config(borderwidth=0)<newline>        self.img.pack(side=""left"")<newline>        self.fld = Text(frm, **fieldParams)<newline>        self.initScrollText(frm, self.fld, initialField)<newline>        frm = Frame(root)<newline>        self.txt = Text(frm, **textParams)<newline>        self.initScrollText(frm, self.txt, initialText)<newline>        for i in range(2):<newline>            self.txt.tag_config(colors[i], background=colors[i])<newline>            self.txt.tag_config(""emph"" + colors[i], foreground=emphColors[i])"
411	adjudicated	5	"def pgcli_bindings(pgcli):<newline>    """"""Custom key bindings for pgcli.""""""<newline>    kb = KeyBindings()"
486	adjudicated	5	"def _concatenate(arrays, axis=0):<newline>    out = np.ma.concatenate(arrays, axis=axis)<newline>    fill_values = [i.fill_value for i in arrays if hasattr(i, ""fill_value"")]<newline>    if any(isinstance(f, np.ndarray) for f in fill_values):<newline>        raise ValueError(<newline>            ""Dask doesn't support masked array's with non-scalar `fill_value`s""<newline>        )<newline>    if fill_values:<newline>        # If all the fill_values are the same copy over the fill value<newline>        fill_values = np.unique(fill_values)<newline>        if len(fill_values) == 1:<newline>            out.fill_value = fill_values[0]<newline>    return out<newline>"
270	adjudicated	5	"class Challenge(jose.TypedJSONObjectWithFields):<newline>    # _fields_to_partial_json<newline>    """"""ACME challenge.""""""<newline>    TYPES: Dict[str, Type['Challenge']] = {}"
464	adjudicated	4	def main():<newline>    args = parser.parse_args()
441	adjudicated	4	"def to_dict(self):<newline>    env = COMMON_ENV_VARIABLES.copy()<newline>    env.update(self.additional_env)<newline>    job = {<newline>        ""working_directory"": self.working_directory,<newline>        ""docker"": self.docker_image,<newline>        ""environment"": env,<newline>    }<newline>    if self.resource_class is not None:<newline>        job[""resource_class""] = self.resource_class<newline>    if self.parallelism is not None:<newline>        job[""parallelism""] = self.parallelism<newline>    steps = [<newline>        ""checkout"",<newline>        {""attach_workspace"": {""at"": ""~/transformers/test_preparation""}},<newline>        {<newline>            ""restore_cache"": {<newline>                ""keys"": [<newline>                    f""v{self.cache_version}-{self.cache_name}-"" + '{{ checksum ""setup.py"" }}',<newline>                    f""v{self.cache_version}-{self.cache_name}-"",<newline>                ]<newline>            }<newline>        },<newline>    ]<newline>    steps.extend([{""run"": l} for l in self.install_steps])<newline>    steps.append(<newline>        {<newline>            ""save_cache"": {<newline>                ""key"": f""v{self.cache_version}-{self.cache_name}-"" + '{{ checksum ""setup.py"" }}',<newline>                ""paths"": [""~/.cache/pip""],<newline>            }<newline>        }<newline>    )<newline>    steps.append({""run"": {""name"": ""Show installed libraries and their versions"",<newline>                          ""command"": ""pip freeze | tee installed.txt""}})<newline>    steps.append({""store_artifacts"": {""path"": ""~/transformers/installed.txt""}})"
276	adjudicated	5	"class RFC3339Field(jose.Field):<newline>    """"""RFC3339 field encoder/decoder.""""""<newline><newline>    def __init__(self, json_name: str, value: Any) -> None:<newline>    self.value = value<newline>    super().__init__(<newline>        json_name=json_name, default=value, omitempty=False)"
398	adjudicated	5	"def _transformers_log_filter(record):<newline>    if record.msg.startswith(""PyTorch version""):<newline>        return False<newline>    return True"
431	adjudicated	4	"def refresh(self, executor, special, callbacks, history=None, settings=None):<newline>    """"""<newline>    Creates a PGCompleter object and populates it with the relevant<newline>    completion suggestions in a background thread.<newline>    """""""
304	adjudicated	4	"def crop_bbox_by_coords(<newline>    bbox: BoxInternalType,<newline>    crop_coords: Tuple[int, int, int, int],<newline>    crop_height: int,<newline>    crop_width: int,<newline>    rows: int,<newline>    cols: int,<newline>):<newline>    """"""Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the<newline>    required height and width of the crop."""""""
345	adjudicated	5	def main():<newline>    project = sys.argv[1]<newline>    json = requests.get('https://pypi.org/pypi/%s/json' % project).json()<newline>    for version in sorted(json['releases'], key=to_tuple):<newline>        print(version)<newline>        wheel_packages = [<newline>            p for p in json['releases'][version]<newline>            if p['packagetype'] == 'bdist_wheel'<newline>        ]<newline>        for p in wheel_packages:<newline>            print('    %(python_version)s %(filename)s' % p)
39	adjudicated	4	"def disable_attention_slicing(self):<newline>    r""""""<newline>    Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go<newline>    back to computing attention in one step.<newline>    """"""<newline>    # set slice_size = `None` to disable `attention slicing`<newline>    self.enable_attention_slicing(None)"
159	adjudicated	5	"def _fake_Popen(*args, **kwargs):<newline>    raise NotImplementedError(""subprocess.Popen is not supported."")"
57	adjudicated	5	"class ClassificationDataset(FlairDataset):<newline>    """"""<newline>    Dataset for classification instantiated from a single FastText-formatted file.<newline>    """""""
482	adjudicated	4	"def pytest_collection_modifyitems(config, items):<newline>    for item in items:<newline>        if ""skip_with_pyarrow_strings"" in item.keywords:<newline>            item.add_marker(skip_with_pyarrow_strings)<newline>        if ""xfail_with_pyarrow_strings"" in item.keywords:<newline>            item.add_marker(xfail_with_pyarrow_strings)"
242	adjudicated	4	class QuietSimpleHTTPServer(SimpleHTTPRequestHandler):<newline>    def log_message(self, *args, **kwargs):<newline>        pass
74	adjudicated	4	def apply_diff(cache_dir: str, diff_file: str, sqlite: bool = False) -> None:<newline>    cache = make_cache(cache_dir, sqlite)<newline>    with open(diff_file) as f:<newline>        diff = json.load(f)
460	adjudicated	4	def bench_flow_call(benchmark: BenchmarkFixture, options):<newline>    noop_flow = flow(**options)(noop_function)<newline>    benchmark(noop_flow)
342	adjudicated	4	"def run(self, value_size, read_size, parser):<newline>    r = self.get_client()<newline>    r.get(""benchmark"")"
26	adjudicated	4	def benchmark_fn():<newline>  for _ in range(100):<newline>    pmap_fn(*sharded_args)<newline>return benchmark_fn
248	adjudicated	4	def main():<newline>    binaries = dict(build_binaries())<newline>    build_packages(binaries['http_cli'], binaries['httpie_cli'])
405	adjudicated	4	class CachedPath(Subcommand):<newline>    requires_plugins: bool = False
162	adjudicated	3	def parse_args(args=None):<newline>    return make_parser().parse_known_args(args)
66	adjudicated	3	"class FlertArguments:<newline>    context_size: int = field(default=0, metadata={""help"": ""Context size when using FLERT approach.""})<newline>    respect_document_boundaries: bool = field(<newline>        default=False,<newline>        metadata={""help"": ""Whether to respect document boundaries or not when using FLERT.""},<newline>    )"
209	adjudicated	4	"def test_transcribe(model_name: str):<newline>    device = ""cuda"" if torch.cuda.is_available() else ""cpu""<newline>    model = whisper.load_model(model_name).to(device)<newline>    audio_path = os.path.join(os.path.dirname(__file__), ""jfk.flac"")"
277	adjudicated	5	def decode(self, value: Any) -> Any:<newline>    if value != self.value:<newline>        raise jose.DeserializationError('Expected {0!r}'.format(self.value))<newline>    return self.value
179	adjudicated	4	"def setup(namespace):<newline>    namespace[""load_response""] = load_response"
33	adjudicated	5	"def check_inputs(self, strength):<newline>    if strength < 0 or strength > 1:<newline>        raise ValueError(f""The value of strength should in [0.0, 1.0] but is {strength}"")"
215	adjudicated	4	class _Undefined:<newline>    def __repr__(self) -> str:<newline>        return 'see-below'
401	adjudicated	5	def _is_empty_default(default: Any) -> bool:<newline>    if default is None:<newline>        return True<newline>    if isinstance(default, (str, list, tuple, set)):<newline>        return not bool(default)<newline>    return False
166	adjudicated	4	def pytest_unconfigure(config):<newline>    from wagtail.test.settings import MEDIA_ROOT, STATIC_ROOT
289	adjudicated	5	"class Identifier(jose.JSONObjectWithFields):<newline>    """"""ACME identifier."""""""
303	adjudicated	4	"def random_crop(img: np.ndarray, crop_height: int, crop_width: int, h_start: float, w_start: float):<newline>    height, width = img.shape[:2]<newline>    if height < crop_height or width < crop_width:<newline>        raise ValueError(<newline>            ""Requested crop size ({crop_height}, {crop_width}) is ""<newline>            ""larger than the image size ({height}, {width})"".format(<newline>                crop_height=crop_height, crop_width=crop_width, height=height, width=width<newline>            )<newline>        )<newline>    x1, y1, x2, y2 = get_random_crop_coords(height, width, crop_height, crop_width, h_start, w_start)<newline>    img = img[y1:y2, x1:x2]<newline>    return img"
261	adjudicated	4	def compare(x, y):<newline>  ss = []<newline>  ln = []<newline>  ln2 = []
437	adjudicated	4	def pytest_terminal_summary(terminalreporter):<newline>    from transformers.testing_utils import pytest_terminal_summary_main
109	adjudicated	4	"def generate_100B_dataset(num_examples: int, chunk_size: int) -> datasets.Dataset:<newline>    table = pa.Table.from_pydict({""col"": [0] * chunk_size})<newline>    table = pa.concat_tables([table] * (num_examples // chunk_size))<newline>    return datasets.Dataset(table, fingerprint=""table_100B"")"
478	adjudicated	4	def main():<newline>    args = parser.parse_args()
470	adjudicated	5	def timestamp(sync=False):<newline>    return time.perf_counter()
38	adjudicated	4	"def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = ""auto""):<newline>    r""""""<newline>    Enable sliced attention computation.<newline>    When this option is enabled, the attention module will split the input tensor in slices, to compute attention<newline>    in several steps. This is useful to save some memory in exchange for a small speed decrease.<newline>    Args:<newline>        slice_size (`str` or `int`, *optional*, defaults to `""auto""`):<newline>            When `""auto""`, halves the input to the attention heads, so attention will be computed in two steps. If<newline>            a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,<newline>            `attention_head_dim` must be a multiple of `slice_size`.<newline>    """"""<newline>    if slice_size == ""auto"":<newline>        # half the attention head size is usually a good trade-off between<newline>        # speed and memory<newline>        slice_size = self.unet.config.attention_head_dim // 2<newline>    self.unet.set_attention_slice(slice_size)"
343	adjudicated	4	def to_int(value):<newline>    value = ''.join((x for x in value if x.isdigit()))<newline>    try:<newline>        return int(value)<newline>    except Exception:<newline>        return 0
91	adjudicated	3	"class PipenvGroup(DYMMixin, Group):<newline>    """"""Custom Group class provides formatted main help"""""""
281	adjudicated	5	"class FakeConnection:<newline>    """"""Fake OpenSSL.SSL.Connection.""""""<newline><newline>    def __init__(self, certs: Mapping[bytes, Tuple[crypto.PKey, crypto.X509]]):<newline>        self.certs = certs<newline><newline>    def __call__(self, connection: SSL.Connection) -> Optional[Tuple[crypto.PKey, crypto.X509]]:<newline>        server_name = connection.get_servername()<newline>        if server_name:<newline>            return self.certs.get(server_name, None)<newline>        return None # pragma: no cover"
218	adjudicated	4	"class PrivateChannel:<newline>    """"""An ABC that details the common operations on a private Discord channel.<newline>    """""""
59	adjudicated	5	"class CSVClassificationDataset(FlairDataset):<newline>    """"""<newline>    Dataset for text classification from CSV column formatted data.<newline>    """""""
397	adjudicated	5	"def build_vocab_from_args(args: argparse.Namespace):<newline>    if not args.output_path.endswith("".tar.gz""):<newline>        raise ValueError(""param 'output_path' should end with '.tar.gz'"")"
370	adjudicated	4	def _bg_refresh(self, sqlexecute, callbacks, completer_options):<newline>    completer = SQLCompleter(**completer_options)
465	adjudicated	4	"def convert_nest(checkpoint_path, arch):<newline>    """"""<newline>    Expects path to checkpoint which is a dir containing 4 files like in each of these folders<newline>        - https://console.cloud.google.com/storage/browser/gresearch/nest-checkpoints<newline>    `arch` is needed to <newline>    Returns a state dict that can be used with `torch.nn.Module.load_state_dict`<newline>    Hint: Follow timm.models.nest.Nest.__init__ and <newline>    https://github.com/google-research/nested-transformer/blob/main/models/nest_net.py<newline>    """"""<newline>    assert arch in ['nest_base', 'nest_small', 'nest_tiny'], ""Your `arch` is not supported"""
410	adjudicated	5	"def add_subparser(self,<newline>                  parser: argparse._SubParsersAction) -> argparse.ArgumentParser:<newline>    description = """"""Dummy command because checklist is not installed.""""""<newline>    subparser = parser.add_parser(<newline>        self.name,<newline>        description=description,<newline>        help=""Run a trained model through a checklist suite."",<newline>    )<newline>    subparser.set_defaults(func=_dummy_output)<newline>    return subparser"
92	adjudicated	4	class State:<newline>    def __init__(self):<newline>        self.index = None<newline>        self.verbose = False<newline>        self.quiet = False<newline>        self.pypi_mirror = None<newline>        self.python = None<newline>        self.site_packages = None<newline>        self.clear = False<newline>        self.system = False<newline>        self.project = Project()<newline>        self.installstate = InstallState()<newline>        self.lockoptions = LockOptions()
351	adjudicated	5	"class HTTPAdapter(BaseAdapter):<newline>    """"""The built-in HTTP Adapter for urllib3.<newline>    """""""
395	adjudicated	4	"class BuildVocab(Subcommand):<newline>    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:<newline>        description = """"""Build a vocabulary from an experiment config file.""""""<newline>        subparser = parser.add_parser(self.name, description=description, help=description)"
336	adjudicated	4	"def parse_args():<newline>    parser = ArgumentParser()<newline>    parser.add_argument(<newline>        ""-n"", type=int, help=""Total number of requests (default 100000)"", default=100000<newline>    )<newline>    parser.add_argument(<newline>        ""-P"",<newline>        type=int,<newline>        help=(""Pipeline <numreq> requests. Default 1 (no pipeline).""),<newline>        default=1,<newline>    )<newline>    parser.add_argument(<newline>        ""-s"",<newline>        type=int,<newline>        help=""Data size of SET/GET value in bytes (default 2)"",<newline>        default=2,<newline>    )"
54	adjudicated	4	"class Entity:<newline>    """"""<newline>    Internal class to represent entities while converting biomedical NER corpora to a standardized format<newline>    (only used for pre-processing purposes!). Each entity consists of the char span it addresses in<newline>    the original text as well as the type of entity (e.g. Chemical, Gene, and so on).<newline>    """""""
382	adjudicated	4	def create_default_config(list_values=True):<newline>    import mycli<newline>    default_config_file = resources.open_text(mycli, 'myclirc')<newline>    return read_config_file(default_config_file, list_values=list_values)
97	adjudicated	5	def upgrade(state, **kwargs):<newline>    from pipenv.routines.update import upgrade<newline>    from pipenv.utils.project import ensure_project
488	adjudicated	4	"def keepdims_wrapper(a_callable):<newline>    """"""<newline>    A wrapper for functions that don't provide keepdims to ensure that they do.<newline>    """""""
341	adjudicated	4	"def setup(self, value_size, read_size, parser):<newline>    r = self.get_client(parser_class=parser, socket_read_size=read_size)<newline>    r.set(""benchmark"", ""a"" * value_size)"
90	adjudicated	4	"def setup(app):<newline>    app.add_css_file(""custom.css"")"
24	adjudicated	5	"def check_python_version(python_version):<newline>  if python_version < (3, 8):<newline>    print(""ERROR: JAX requires Python 3.8 or newer, found "", python_version)<newline>    sys.exit(-1)"
240	adjudicated	4	"def register():<newline>    try:<newline>        params = request.json<newline>        host = params.get(""host"")<newline>        port = params.get(""port"")<newline>        _type = params.get(""type"")<newline>        Cache[(host, port)] = _type<newline>        return ""OK"", 200<newline>    except Exception as e:<newline>        return str(e), 500"
185	adjudicated	4	"def maybe_skip_member(app, what, name, obj, skip, options):<newline>    if not skip:<newline>        # autodocs was generating a text ""alias of"" for the following members<newline>        # https://github.com/sphinx-doc/sphinx/issues/4422<newline>        return name in {""default_item_class"", ""default_selector_class""}<newline>    return skip"
480	adjudicated	4	"def pytest_addoption(parser):<newline>    parser.addoption(""--runslow"", action=""store_true"", help=""run slow tests"")"
325	adjudicated	4	"def get_client(self, **kwargs):<newline>    # eventually make this more robust and take optional args from<newline>    # argparse<newline>    if self._client is None or kwargs:<newline>        defaults = {""db"": 9}<newline>        defaults.update(kwargs)<newline>        pool = redis.ConnectionPool(**kwargs)<newline>        self._client = redis.Redis(connection_pool=pool)<newline>    return self._client"
221	adjudicated	3	class Bot(commands.{base}):<newline>    def __init__(self, intents: discord.Intents, **kwargs):<newline>        super().__init__(command_prefix=commands.when_mentioned_or('{prefix}'), intents=intents, **kwargs)
496	adjudicated	5	"class NotThisMethod(Exception):<newline>    """"""Exception raised if a method is not valid for the current scenario."""""""
403	adjudicated	4	"def parse_args(prog: Optional[str] = None) -> Tuple[argparse.ArgumentParser, argparse.Namespace]:<newline>    """"""<newline>    Creates the argument parser for the main program and uses it to parse the args.<newline>    """"""<newline>    parser = ArgumentParserWithDefaults(description=""Run AllenNLP"", prog=prog)<newline>    parser.add_argument(""--version"", action=""version"", version=f""%(prog)s {__version__}"")"
285	adjudicated	5	"class ClientError(Error):<newline>    """"""Network error."""""""
52	adjudicated	4	"def ddpm_bit_scheduler_step(<newline>    self,<newline>    model_output: torch.FloatTensor,<newline>    timestep: int,<newline>    sample: torch.FloatTensor,<newline>    prediction_type=""epsilon"",<newline>    generator=None,<newline>    return_dict: bool = True,<newline>) -> Union[DDPMSchedulerOutput, Tuple]:<newline>    """"""<newline>    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion<newline>    process from the learned model outputs (most often the predicted noise).<newline>    Args:<newline>        model_output (`torch.FloatTensor`): direct output from learned diffusion model.<newline>        timestep (`int`): current discrete timestep in the diffusion chain.<newline>        sample (`torch.FloatTensor`):<newline>            current instance of sample being created by diffusion process.<newline>        prediction_type (`str`, default `epsilon`):<newline>            indicates whether the model predicts the noise (epsilon), or the samples (`sample`).<newline>        generator: random number generator.<newline>        return_dict (`bool`): option for returning tuple rather than DDPMSchedulerOutput class<newline>    Returns:<newline>        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] or `tuple`:<newline>        [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When<newline>        returning a tuple, the first element is the sample tensor.<newline>    """"""<newline>    t = timestep"
238	adjudicated	5	"async def wait_apis_start():<newline>    futures = [<newline>        wait_api_start(api_name, api_data['process'].pid, api_data['port'])<newline>        for api_name, api_data in apis.items() if 'port' in api_data<newline>    ]<newline>    for i, future in enumerate(asyncio.as_completed(futures)):<newline>        api_name, port, started = await future<newline>        if started:<newline>            print(f""{api_name} API: started on {port}"")<newline>        else:<newline>            log.logger.error(f""ERROR: {api_name} API cant start on {port}"")"
328	adjudicated	4	"class StringJoiningConnection(Connection):<newline>    def send_packed_command(self, command, check_health=True):<newline>        ""Send an already packed command to the Redis server""<newline>        if not self._sock:<newline>            self.connect()<newline>        try:<newline>            self._sock.sendall(command)<newline>        except OSError as e:<newline>            self.disconnect()<newline>            if len(e.args) == 1:<newline>                _errno, errmsg = ""UNKNOWN"", e.args[0]<newline>            else:<newline>                _errno, errmsg = e.args<newline>            raise ConnectionError(f""Error {_errno} while writing to socket. {errmsg}."")<newline>        except Exception:<newline>            self.disconnect()<newline>            raise"
456	adjudicated	4	"def reset_object_registry():<newline>    """"""<newline>    Ensures each test has a clean object registry.<newline>    """"""<newline>    from prefect.context import PrefectObjectRegistry"
287	adjudicated	5	"class _Constant(jose.JSONDeSerializable, Hashable):<newline>    """"""ACME constant.""""""<newline>    __slots__ = ('name',)<newline>    POSSIBLE_NAMES: Dict[str, '_Constant'] = NotImplemented"
323	adjudicated	4	"class Benchmark:<newline>    ARGUMENTS = ()<newline><newline>    def __init__(self):<newline>        self._client = None<newline><newline>    def get_client(self, **kwargs):<newline>        # eventually make this more robust and take optional args from<newline>        # argparse<newline>        if self._client is None or kwargs:<newline>            defaults = {""db"": 9}<newline>            defaults.update(kwargs)<newline>            pool = redis.ConnectionPool(**kwargs)<newline>            self._client = redis.Redis(connection_pool=pool)<newline>        return self._client<newline><newline>     def setup(self, **kwargs):<newline>        pass<newline><newline>    def run(self, **kwargs):<newline>        pass"
12	adjudicated	3	def get_cluster_username(region):<newline>    return requests.post(_API_URL, json={'name':'get_cluster_username', 'region':region},<newline>                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['cluster_username']
203	adjudicated	4	"def test_number_normalizer(std):<newline>    assert std(""two"") == ""2""<newline>    assert std(""thirty one"") == ""31""<newline>    assert std(""five twenty four"") == ""524""<newline>    assert std(""nineteen ninety nine"") == ""1999""<newline>    assert std(""twenty nineteen"") == ""2019"""
432	adjudicated	3	def is_refreshing(self):<newline>    return self._completer_thread and self._completer_thread.is_alive()
348	adjudicated	5	class MyCorpus:<newline>    def __iter__(self):<newline>        for line in open('https://radimrehurek.com/mycorpus.txt'):<newline>            # assume there's one document per line, tokens separated by whitespace<newline>            yield dictionary.doc2bow(line.lower().split())
433	adjudicated	4	def _bg_refresh(self, pgexecute, special, callbacks, history=None, settings=None):<newline>    settings = settings or {}<newline>    completer = PGCompleter(<newline>        smart_completion=True, pgspecial=special, settings=settings<newline>    )
249	adjudicated	5	"class FinishedForNow(Exception):<newline>    """"""Raised when remaining GitHub rate limit is zero.""""""<newline><newline>    def main(previous_release: str, current_release: str) -> int:<newline>    since = release_date(previous_release)<newline>    until = release_date(current_release)"
305	adjudicated	4	def bbox_random_crop(<newline>    bbox: BoxInternalType, crop_height: int, crop_width: int, h_start: float, w_start: float, rows: int, cols: int<newline>):<newline>    crop_coords = get_random_crop_coords(rows, cols, crop_height, crop_width, h_start, w_start)<newline>    return crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols)
414	adjudicated	5	"def _(event):<newline>    """"""Toggle between Vi and Emacs mode.""""""<newline>    _logger.debug(""Detected F4 key."")<newline>    pgcli.vi_mode = not pgcli.vi_mode<newline>    event.app.editing_mode = EditingMode.VI if pgcli.vi_mode else EditingMode.EMACS"
175	adjudicated	4	"def validate_extra_context(ctx, param, value):<newline>    """"""Validate extra context.""""""<newline>    for string in value:<newline>        if '=' not in string:<newline>            raise click.BadParameter(<newline>                f""EXTRA_CONTEXT should contain items of the form key=value; ""<newline>                f""'{string}' doesn't match that form""<newline>            )"
275	adjudicated	5	"class Fixed(jose.Field):<newline>    """"""Fixed field."""""""
206	adjudicated	4	"def test_audio():<newline>    audio_path = os.path.join(os.path.dirname(__file__), ""jfk.flac"")<newline>    audio = load_audio(audio_path)<newline>    assert audio.ndim == 1<newline>    assert SAMPLE_RATE * 10 < audio.shape[0] < SAMPLE_RATE * 12<newline>    assert 0 < audio.std() < 1"
174	adjudicated	5	"def version_msg():<newline>    """"""Return the Cookiecutter version, location and Python powering it.""""""<newline>    python_version = sys.version<newline>    location = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))<newline>    return f""Cookiecutter {__version__} from {location} (Python {python_version})"""
102	adjudicated	5	"def cert_parse_args(self, args):<newline>    if not self.parser.get_default_values().cert:<newline>        # There are no user provided cert -- force use of bundled cert<newline>        self.parser.defaults[""cert""] = cert_path  # calculated above<newline>    return install_parse_args(self, args)"
30	adjudicated	4	"class CLIPGuidedStableDiffusion(DiffusionPipeline):<newline>    """"""CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000<newline>    - https://github.com/Jack000/glid-3-xl<newline>    - https://github.dev/crowsonkb/k-diffusion<newline>    """"""<newline><newline>    def __init__(self, cut_size, cut_power=1.0):<newline>        super().__init__()<newline><newline>    def forward(self, pixel_values, num_cutouts):<newline>    sideY, sideX = pixel_values.shape[2:4]<newline>    max_size = min(sideX, sideY)<newline>    min_size = min(sideX, sideY, self.cut_size)<newline>    cutouts = []<newline>    for _ in range(num_cutouts):<newline>        size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)<newline>        offsetx = torch.randint(0, sideX - size + 1, ())<newline>        offsety = torch.randint(0, sideY - size + 1, ())<newline>        cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]<newline>        cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))<newline>    return torch.cat(cutouts)<newline><newline>    def spherical_dist_loss(x, y):<newline>    x = F.normalize(x, dim=-1)<newline>    y = F.normalize(y, dim=-1)<newline>    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)"
235	adjudicated	5	"def update_rate_limit(self, current: Optional[float] = None, *, tokens: int = 1) -> Optional[float]:<newline>    """"""Updates the cooldown rate limit.<newline>    """""""
114	adjudicated	4	def read(dataset: datasets.Dataset, length):<newline>    for i in range(length):<newline>        _ = dataset[i]
40	adjudicated	3	"class ComposableStableDiffusionPipeline(DiffusionPipeline):<newline>    r""""""<newline>    Pipeline for text-to-image generation using Stable Diffusion."
2	adjudicated	4	"def _get_baseline_means(baseline_dir, name):<newline>  baseline_dir = os.path.expanduser(baseline_dir)<newline>  filename = os.path.join(baseline_dir, name + "".csv"")<newline>  if not os.path.exists(filename):<newline>    raise FileNotFoundError(""Can't find baseline file: %s"" % filename)<newline>  with open(filename, newline="""") as csvfile:<newline>    reader = csv.reader(csvfile)<newline>    header = next(reader)<newline>    mean_idx = header.index(""mean"")<newline>    return [float(row[mean_idx]) for row in reader]"
497	adjudicated	5	"def get_keywords():<newline>    """"""Get the keywords needed to look up the version information.""""""<newline>    # these strings will be replaced by git during git-archive.<newline>    # setup.py/versioneer.py will grep for the variable names, so they must<newline>    # each be defined on a line of their own. _version.py will just call<newline>    # get_keywords().<newline>    git_refnames = ""$Format:%d$""<newline>    git_full = ""$Format:%H$""<newline>    keywords = {""refnames"": git_refnames, ""full"": git_full}<newline>    return keywords"
168	adjudicated	4	"def setup(app):<newline>    app.add_js_file(""js/banner.js"")"
440	adjudicated	4	def __post_init__(self):<newline>    # Deal with defaults for mutable attributes.<newline>    if self.additional_env is None:<newline>        self.additional_env = {}<newline>    if self.cache_name is None:<newline>        self.cache_name = self.name<newline>    if self.docker_image is None:<newline>        # Let's avoid changing the default list and make a copy.<newline>        self.docker_image = copy.deepcopy(DEFAULT_DOCKER_IMAGE)<newline>    if self.install_steps is None:<newline>        self.install_steps = []<newline>    if self.pytest_options is None:<newline>        self.pytest_options = {}<newline>    if isinstance(self.tests_to_run, str):<newline>        self.tests_to_run = [self.tests_to_run]<newline>    if self.parallelism is None:<newline>        self.parallelism = 1
322	adjudicated	4	"async def get_str(client, gather):<newline>    if gather:<newline>        for _ in range(count // 100):<newline>            await asyncio.gather(<newline>                *(asyncio.create_task(client.get(f""bench:str_{i}"")) for i in range(100))<newline>            )<newline>    else:<newline>        for i in range(count):<newline>            await client.get(f""bench:str_{i}"")"
279	adjudicated	5	class _DefaultCertSelection:<newline>    def __init__(self, certs: Mapping[bytes, Tuple[crypto.PKey, crypto.X509]]):<newline>        self.certs = certs
153	adjudicated	4	def __init__(self):<newline>    self.queue = q.Queue()<newline>    self.model = CollocationsModel(self.queue)<newline>    self.top = Tk()<newline>    self._init_top(self.top)<newline>    self._init_menubar()<newline>    self._init_widgets(self.top)<newline>    self.load_corpus(self.model.DEFAULT_CORPUS)<newline>    self.after = self.top.after(POLL_INTERVAL, self._poll)
131	adjudicated	3	def train_test_split(dataset: datasets.Dataset):<newline>    _ = dataset.train_test_split(0.1)
483	adjudicated	4	"def shuffle_method(request):<newline>    with dask.config.set({""dataframe.shuffle.method"": request.param}):<newline>        yield request.param"
223	adjudicated	4	def core(parser: argparse.ArgumentParser, args: argparse.Namespace) -> None:<newline>    if args.version:<newline>        show_version()<newline>    else:<newline>        parser.print_help()
250	adjudicated	4	def find_committers(since: str, until: str) -> FullNames:<newline>    url = f'{REPO_URL}/commits'<newline>    page = 1<newline>    per_page = 100<newline>    params = {<newline>        'since': since,<newline>        'until': until,<newline>        'per_page': per_page,<newline>    }<newline>    committers: FullNames = set()
36	adjudicated	5	"class ImagicStableDiffusionPipeline(DiffusionPipeline):<newline>    r""""""<newline>    Pipeline for imagic image editing.<newline>    See paper here: https://arxiv.org/pdf/2210.09276.pdf<newline>    """""""
233	adjudicated	5	"def get_tokens(self, current: Optional[float] = None) -> int:<newline>    """"""Returns the number of available tokens before rate limiting is applied.<newline>    """""""
283	adjudicated	5	"class DependencyError(Error):<newline>    """"""Dependency error"""""""
10	adjudicated	4	def find_existing_cluster():<newline>    return requests.post(_API_URL, json={'name':'find_cluster'},<newline>                         auth=_API_AUTH, timeout=_REQUEST_TIMEOUT_SECONDS).json()['region']
190	adjudicated	4	def parse(self, response):<newline>    pass
267	adjudicated	4	def data(self):<newline>  data = libane.ANE_TensorData(self.tt)<newline>  assert(data is not None)<newline>  #print(hex(addressof(data.contents)))<newline>  buf = np.ctypeslib.as_array(data, shape=(self.sz,))<newline>  ret = np.frombuffer(buf, dtype=self.dtype)<newline>  #print(ret.data)<newline>  return ret
438	adjudicated	4	def pytest_sessionfinish(session, exitstatus):<newline>    # If no tests are collected, pytest exists with code 5, which makes the CI fail.<newline>    if exitstatus == 5:<newline>        session.exitstatus = 0
274	adjudicated	5	"class KeyAuthorizationChallengeResponse(ChallengeResponse):<newline>    """"""Response to Challenges based on Key Authorization.<newline>    """""""
332	adjudicated	4	"async def wrapper(*args, **kwargs):<newline>    tic = time.perf_counter()<newline>    await func(*args, **kwargs)<newline>    toc = time.perf_counter()<newline>    return f""{toc - tic:.4f}"""
125	adjudicated	3	"def read_formatted_as_numpy(feats, tmp_dir):<newline>    dataset = datasets.Dataset.from_file(<newline>        filename=os.path.join(tmp_dir, ""beta.arrow""), info=datasets.DatasetInfo(features=feats)<newline>    )<newline>    dataset.set_format(""numpy"")<newline>    for _ in dataset:<newline>        pass"
449	adjudicated	4	"def hello(name: str = ""world""):<newline>    prefect.get_run_logger().info(f""Hello {name}!"")"
194	adjudicated	4	class Root(Resource):<newline>    def __init__(self):<newline>        Resource.__init__(self)<newline>        self.concurrent = 0<newline>        self.tail = deque(maxlen=100)<newline>        self._reset_stats()
69	adjudicated	4	"class Dictionary:<newline>    """"""<newline>    This class holds a dictionary that maps strings to IDs, used to generate one-hot encodings of strings.<newline>    """""""
202	adjudicated	4	def test_median_filter_equivalence(shape):<newline>    x = torch.randn(*shape)
269	adjudicated	5	"def get_pid(name):<newline>  try:<newline>    output = check_output([""pgrep"", name])<newline>    return int(output)<newline>  except:<newline>    return None"
315	adjudicated	5	def __init__(self, argv: Optional[str] = None) -> None:<newline>    self.argv = argv or sys.argv[:]<newline>    self.prog_name = Path(self.argv[0]).name
419	adjudicated	4	"def config_location():<newline>    if ""XDG_CONFIG_HOME"" in os.environ:<newline>        return ""%s/pgcli/"" % expanduser(os.environ[""XDG_CONFIG_HOME""])<newline>    elif platform.system() == ""Windows"":<newline>        return os.getenv(""USERPROFILE"") + ""\\AppData\\Local\\dbcli\\pgcli\\""<newline>    else:<newline>        return expanduser(""~/.config/pgcli/"")"
455	adjudicated	5	def bench_task_submit(benchmark: BenchmarkFixture, num_task_runs: int):<newline>    noop_task = task(noop_function)
479	adjudicated	4	"def clean_checkpoint(<newline>        checkpoint,<newline>        output,<newline>        use_ema=True,<newline>        no_hash=False,<newline>        clean_aux_bn=False,<newline>        safe_serialization: bool=False,<newline>):<newline>    # Load an existing checkpoint to CPU, strip everything but the state_dict and re-save<newline>    if checkpoint and os.path.isfile(checkpoint):<newline>        print(""=> Loading checkpoint '{}'"".format(checkpoint))<newline>        state_dict = load_state_dict(checkpoint, use_ema=use_ema)<newline>        new_state_dict = {}<newline>        for k, v in state_dict.items():<newline>            if clean_aux_bn and 'aux_bn' in k:<newline>                # If all aux_bn keys are removed, the SplitBN layers will end up as normal and<newline>                # load with the unmodified classifier using BatchNorm2d.<newline>                continue<newline>            name = k[7:] if k.startswith('module.') else k<newline>            new_state_dict[name] = v<newline>        print(""=> Loaded state_dict from '{}'"".format(checkpoint))"
485	adjudicated	3	"def percentile(a, q, method=""linear""):<newline>    return _percentile(a, q, method)"
491	adjudicated	5	"def trim(x, axes=None):<newline>    """"""Trim boundaries off of array<newline>    """""""
498	adjudicated	5	"def get_config():<newline>    """"""Create, populate and return the VersioneerConfig() object.""""""<newline>    # these strings are filled in when 'setup.py versioneer' creates<newline>    # _version.py<newline>    cfg = VersioneerConfig()<newline>    cfg.VCS = ""git""<newline>    cfg.style = ""pep440""<newline>    cfg.tag_prefix = """"<newline>    cfg.parentdir_prefix = ""dask-""<newline>    cfg.versionfile_source = ""dask/_version.py""<newline>    cfg.verbose = False<newline>    return cfg"
75	adjudicated	4	"def main() -> None:<newline>    parser = argparse.ArgumentParser()<newline>    parser.add_argument(""--sqlite"", action=""store_true"", default=False, help=""Use a sqlite cache"")<newline>    parser.add_argument(""cache_dir"", help=""Directory for the cache"")<newline>    parser.add_argument(""diff"", help=""Cache diff file"")<newline>    args = parser.parse_args()"
165	adjudicated	4	"def pytest_configure(config):<newline>    deprecation = config.getoption(""deprecation"")"
471	adjudicated	4	"def cmd_from_args(args) -> Tuple[Union[Callable, str], List[str]]:<newline>    # If ``args`` not passed, defaults to ``sys.argv[:1]``<newline>    with_python = not args.no_python<newline>    cmd: Union[Callable, str]<newline>    cmd_args = []<newline>    if with_python:<newline>        cmd = os.getenv(""PYTHON_EXEC"", sys.executable)<newline>        cmd_args.append(""-u"")<newline>        if args.module:<newline>            cmd_args.append(""-m"")<newline>        cmd_args.append(args.script)<newline>    else:<newline>        if args.module:<newline>            raise ValueError(<newline>                ""Don't use both the '--no_python' flag""<newline>                "" and the '--module' flag at the same time.""<newline>            )<newline>        cmd = args.script<newline>    cmd_args.extend(args.script_args)"
254	adjudicated	4	def generate_documentation() -> str:<newline>    database = load_database()<newline>    structure = build_docs_structure(database)<newline>    template = Template(source=TPL_FILE.read_text(encoding='utf-8'))<newline>    output = template.render(structure=structure)<newline>    output = clean_template_output(output)<newline>    return output