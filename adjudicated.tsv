UID	Category	Snippet
huggingface|transformers|conftest.py|000	Class	"class CustomOutputChecker(OutputChecker):
    def check_output(self, want, got, optionflags):
        if IGNORE_RESULT & optionflags:
            return True
        return OutputChecker.check_output(self, want, got, optionflags)
"
huggingface|transformers|conftest.py|001	Function	"def pytest_configure(config):
    config.addinivalue_line(
        ""markers"", ""is_pt_tf_cross_test: mark test to run only when PT and TF interactions are tested""
    )
    config.addinivalue_line(
        ""markers"", ""is_pt_flax_cross_test: mark test to run only when PT and FLAX interactions are tested""
    )
    config.addinivalue_line(
        ""markers"", ""is_pipeline_test: mark test to run only when pipelines are tested""
    )
    config.addinivalue_line(""markers"", ""is_staging_test: mark test to run only in the staging environment"")
"
huggingface|transformers|conftest.py|002	Function	"def pytest_addoption(parser):
    from transformers.testing_utils import pytest_addoption_shared
"
huggingface|transformers|conftest.py|003	Function	"def pytest_terminal_summary(terminalreporter):
    from transformers.testing_utils import pytest_terminal_summary_main
"
huggingface|transformers|conftest.py|004	Function	"def pytest_sessionfinish(session, exitstatus):
    # If no tests are collected, pytest exists with code 5, which makes the CI fail.
    if exitstatus == 5:
        session.exitstatus = 0
"
huggingface|transformers|conftest.py|005	Function	"    def check_output(self, want, got, optionflags):
        if IGNORE_RESULT & optionflags:
            return True
        return OutputChecker.check_output(self, want, got, optionflags)
"
huggingface|transformers|hubconf.py|000	Function	"def config(*args, **kwargs):
    r""""""
                # Using torch.hub !
                import torch
"
huggingface|transformers|hubconf.py|001	Function	"def tokenizer(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch
"
huggingface|transformers|hubconf.py|002	Function	"def model(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch
"
huggingface|transformers|hubconf.py|003	Function	"def modelForCausalLM(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch
"
huggingface|transformers|hubconf.py|004	Function	"def modelForMaskedLM(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch
"
huggingface|transformers|hubconf.py|005	Function	"def modelForSequenceClassification(*args, **kwargs):
    r""""""
            # Using torch.hub !
            import torch
"
huggingface|transformers|hubconf.py|006	Function	"def modelForQuestionAnswering(*args, **kwargs):
    r""""""
        # Using torch.hub !
        import torch
"
huggingface|transformers|setup.py|000	Class	"class DepsTableUpdateCommand(Command):
    """"""
    A custom distutils command that updates the dependency table.
    usage: python setup.py deps_table_update
    """"""
"
huggingface|transformers|setup.py|001	Function	"def deps_list(*pkgs):
    return [deps[pkg] for pkg in pkgs]
"
huggingface|transformers|setup.py|002	Function	"    def initialize_options(self):
        pass
"
huggingface|transformers|setup.py|003	Function	"    def finalize_options(self):
        pass
"
huggingface|transformers|setup.py|004	Function	"    def run(self):
        entries = ""\n"".join([f'    ""{k}"": ""{v}"",' for k, v in deps.items()])
        content = [
            ""# THIS FILE HAS BEEN AUTOGENERATED. To update:"",
            ""# 1. modify the `_deps` dict in setup.py"",
            ""# 2. run `make deps_table_update``"",
            ""deps = {"",
            entries,
            ""}"",
            """",
        ]
        target = ""src/transformers/dependency_versions_table.py""
        print(f""updating {target}"")
        with open(target, ""w"", encoding=""utf-8"", newline=""\n"") as f:
            f.write(""\n"".join(content))
"
psf|requests|setup.py|000	Class	"class PyTest(TestCommand):
    user_options = [(""pytest-args="", ""a"", ""Arguments to pass into py.test"")]
"
psf|requests|setup.py|001	Function	"    def initialize_options(self):
        TestCommand.initialize_options(self)
        try:
            from multiprocessing import cpu_count
"
psf|requests|setup.py|002	Function	"    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True
"
psf|requests|setup.py|003	Function	"    def run_tests(self):
        import pytest
"
scrapy|scrapy|conftest.py|000	Function	"def _py_files(folder):
    return (str(p) for p in Path(folder).rglob(""*.py""))
"
scrapy|scrapy|conftest.py|001	Function	"def chdir(tmpdir):
    """"""Change to pytest-provided temporary directory""""""
    tmpdir.chdir()
"
scrapy|scrapy|conftest.py|002	Function	"def pytest_addoption(parser):
    parser.addoption(
        ""--reactor"",
        default=""default"",
        choices=[""default"", ""asyncio""],
    )
"
scrapy|scrapy|conftest.py|003	Function	"def reactor_pytest(request):
    if not request.cls:
        # doctests
        return
    request.cls.reactor_pytest = request.config.getoption(""--reactor"")
    return request.cls.reactor_pytest
"
scrapy|scrapy|conftest.py|004	Function	"def only_asyncio(request, reactor_pytest):
    if request.node.get_closest_marker(""only_asyncio"") and reactor_pytest != ""asyncio"":
        pytest.skip(""This test is only run with --reactor=asyncio"")
"
scrapy|scrapy|conftest.py|005	Function	"def only_not_asyncio(request, reactor_pytest):
    if (
        request.node.get_closest_marker(""only_not_asyncio"")
        and reactor_pytest == ""asyncio""
    ):
        pytest.skip(""This test is only run without --reactor=asyncio"")
"
scrapy|scrapy|conftest.py|006	Function	"def pytest_configure(config):
    if config.getoption(""--reactor"") == ""asyncio"":
        install_reactor(""twisted.internet.asyncioreactor.AsyncioSelectorReactor"")
"
scrapy|scrapy|setup.py|000	Function	"def has_environment_marker_platform_impl_support():
    """"""Code extracted from 'pytest/setup.py'
    https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31
"
openai|whisper|setup.py|000	Function	"def read_version(fname=""whisper/version.py""):
    exec(compile(open(fname, encoding=""utf-8"").read(), fname, ""exec""))
    return locals()[""__version__""]
"
httpie|httpie|setup.py|000	Function	"def long_description():
    with open('README.md', encoding='utf-8') as f:
        return f.read()
"
huggingface|pytorch-image-models|avg_checkpoints.py|000	Function	"def checkpoint_metric(checkpoint_path):
    if not checkpoint_path or not os.path.isfile(checkpoint_path):
        return {}
    print(""=> Extracting metric from checkpoint '{}'"".format(checkpoint_path))
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    metric = None
    if 'metric' in checkpoint:
        metric = checkpoint['metric']
    elif 'metrics' in checkpoint and 'metric_name' in checkpoint:
        metrics = checkpoint['metrics']
        print(metrics)
        metric = metrics[checkpoint['metric_name']]
    return metric
"
huggingface|pytorch-image-models|avg_checkpoints.py|001	Function	"def main():
    args = parser.parse_args()
    # by default use the EMA weights (if present)
    args.use_ema = not args.no_use_ema
    # by default sort by checkpoint metric (if present) and avg top n checkpoints
    args.sort = not args.no_sort
"
huggingface|pytorch-image-models|benchmark.py|000	Class	"class BenchmarkRunner:
    def __init__(
            self,
            model_name,
            detail=False,
            device='cuda',
            torchscript=False,
            torchcompile=None,
            aot_autograd=False,
            precision='float32',
            fuser='',
            num_warm_iter=10,
            num_bench_iter=50,
            use_train_size=False,
            **kwargs
    ):
        self.model_name = model_name
        self.detail = detail
        self.device = device
        self.amp_dtype, self.model_dtype, self.data_dtype = resolve_precision(precision)
        self.channels_last = kwargs.pop('channels_last', False)
        if self.amp_dtype is not None:
            self.amp_autocast = partial(torch.cuda.amp.autocast, dtype=self.amp_dtype)
        else:
            self.amp_autocast = suppress
"
huggingface|pytorch-image-models|benchmark.py|001	Class	"class InferenceBenchmarkRunner(BenchmarkRunner):
"
huggingface|pytorch-image-models|benchmark.py|002	Class	"class TrainBenchmarkRunner(BenchmarkRunner):
"
huggingface|pytorch-image-models|benchmark.py|003	Class	"class ProfileRunner(BenchmarkRunner):
"
huggingface|pytorch-image-models|benchmark.py|004	Function	"def timestamp(sync=False):
    return time.perf_counter()
"
huggingface|pytorch-image-models|benchmark.py|005	Function	"def cuda_timestamp(sync=False, device=None):
    if sync:
        torch.cuda.synchronize(device=device)
    return time.perf_counter()
"
huggingface|pytorch-image-models|benchmark.py|006	Function	"def count_params(model: nn.Module):
    return sum([m.numel() for m in model.parameters()])
"
huggingface|pytorch-image-models|benchmark.py|007	Function	"def resolve_precision(precision: str):
    assert precision in ('amp', 'amp_bfloat16', 'float16', 'bfloat16', 'float32')
    amp_dtype = None  # amp disabled
    model_dtype = torch.float32
    data_dtype = torch.float32
    if precision == 'amp':
        amp_dtype = torch.float16
    elif precision == 'amp_bfloat16':
        amp_dtype = torch.bfloat16
    elif precision == 'float16':
        model_dtype = torch.float16
        data_dtype = torch.float16
    elif precision == 'bfloat16':
        model_dtype = torch.bfloat16
        data_dtype = torch.bfloat16
    return amp_dtype, model_dtype, data_dtype
"
huggingface|pytorch-image-models|benchmark.py|008	Function	"def profile_deepspeed(model, input_size=(3, 224, 224), batch_size=1, detailed=False):
    _, macs, _ = get_model_profile(
        model=model,
        input_shape=(batch_size,) + input_size,  # input shape/resolution
        print_profile=detailed,  # prints the model graph with the measured profile attached to each module
        detailed=detailed,  # print the detailed profile
        warm_up=10,  # the number of warm-ups before measuring the time of each module
        as_string=False,  # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)
        output_file=None,  # path to the output file. If None, the profiler prints to stdout.
        ignore_modules=None)  # the list of modules to ignore in the profiling
    return macs, 0  # no activation count in DS
"
huggingface|pytorch-image-models|benchmark.py|009	Function	"def profile_fvcore(model, input_size=(3, 224, 224), batch_size=1, detailed=False, force_cpu=False):
    if force_cpu:
        model = model.to('cpu')
    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype
    example_input = torch.ones((batch_size,) + input_size, device=device, dtype=dtype)
    fca = FlopCountAnalysis(model, example_input)
    aca = ActivationCountAnalysis(model, example_input)
    if detailed:
        fcs = flop_count_str(fca)
        print(fcs)
    return fca.total(), aca.total()
"
huggingface|pytorch-image-models|benchmark.py|010	Function	"    def __init__(
            self,
            model_name,
            detail=False,
            device='cuda',
            torchscript=False,
            torchcompile=None,
            aot_autograd=False,
            precision='float32',
            fuser='',
            num_warm_iter=10,
            num_bench_iter=50,
            use_train_size=False,
            **kwargs
    ):
        self.model_name = model_name
        self.detail = detail
        self.device = device
        self.amp_dtype, self.model_dtype, self.data_dtype = resolve_precision(precision)
        self.channels_last = kwargs.pop('channels_last', False)
        if self.amp_dtype is not None:
            self.amp_autocast = partial(torch.cuda.amp.autocast, dtype=self.amp_dtype)
        else:
            self.amp_autocast = suppress
"
huggingface|pytorch-image-models|benchmark.py|011	Function	"    def _init_input(self):
        self.example_inputs = torch.randn(
            (self.batch_size,) + self.input_size, device=self.device, dtype=self.data_dtype)
        if self.channels_last:
            self.example_inputs = self.example_inputs.contiguous(memory_format=torch.channels_last)
"
huggingface|pytorch-image-models|benchmark.py|012	Function	"    def __init__(
            self,
            model_name,
            device='cuda',
            torchscript=False,
            **kwargs
    ):
        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)
        self.model.eval()
"
huggingface|pytorch-image-models|benchmark.py|013	Function	"    def run(self):
        def _step():
            t_step_start = self.time_fn()
            with self.amp_autocast():
                output = self.model(self.example_inputs)
            t_step_end = self.time_fn(True)
            return t_step_end - t_step_start
"
huggingface|pytorch-image-models|benchmark.py|014	Function	"        def _step():
            t_step_start = self.time_fn()
            with self.amp_autocast():
                output = self.model(self.example_inputs)
            t_step_end = self.time_fn(True)
            return t_step_end - t_step_start
"
huggingface|pytorch-image-models|benchmark.py|015	Function	"    def __init__(
            self,
            model_name,
            device='cuda',
            torchscript=False,
            **kwargs
    ):
        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)
        self.model.train()
"
huggingface|pytorch-image-models|benchmark.py|016	Function	"    def _gen_target(self, batch_size):
        return torch.empty(
            (batch_size,) + self.target_shape, device=self.device, dtype=torch.long).random_(self.num_classes)
"
huggingface|pytorch-image-models|benchmark.py|017	Function	"    def run(self):
        def _step(detail=False):
            self.optimizer.zero_grad()  # can this be ignored?
            t_start = self.time_fn()
            t_fwd_end = t_start
            t_bwd_end = t_start
            with self.amp_autocast():
                output = self.model(self.example_inputs)
                if isinstance(output, tuple):
                    output = output[0]
                if detail:
                    t_fwd_end = self.time_fn(True)
                target = self._gen_target(output.shape[0])
                self.loss(output, target).backward()
                if detail:
                    t_bwd_end = self.time_fn(True)
            self.optimizer.step()
            t_end = self.time_fn(True)
            if detail:
                delta_fwd = t_fwd_end - t_start
                delta_bwd = t_bwd_end - t_fwd_end
                delta_opt = t_end - t_bwd_end
                return delta_fwd, delta_bwd, delta_opt
            else:
                delta_step = t_end - t_start
                return delta_step
"
huggingface|pytorch-image-models|benchmark.py|018	Function	"        def _step(detail=False):
            self.optimizer.zero_grad()  # can this be ignored?
            t_start = self.time_fn()
            t_fwd_end = t_start
            t_bwd_end = t_start
            with self.amp_autocast():
                output = self.model(self.example_inputs)
                if isinstance(output, tuple):
                    output = output[0]
                if detail:
                    t_fwd_end = self.time_fn(True)
                target = self._gen_target(output.shape[0])
                self.loss(output, target).backward()
                if detail:
                    t_bwd_end = self.time_fn(True)
            self.optimizer.step()
            t_end = self.time_fn(True)
            if detail:
                delta_fwd = t_fwd_end - t_start
                delta_bwd = t_bwd_end - t_fwd_end
                delta_opt = t_end - t_bwd_end
                return delta_fwd, delta_bwd, delta_opt
            else:
                delta_step = t_end - t_start
                return delta_step
"
huggingface|pytorch-image-models|benchmark.py|019	Function	"    def __init__(self, model_name, device='cuda', profiler='', **kwargs):
        super().__init__(model_name=model_name, device=device, **kwargs)
        if not profiler:
            if has_deepspeed_profiling:
                profiler = 'deepspeed'
            elif has_fvcore_profiling:
                profiler = 'fvcore'
        assert profiler, ""One of deepspeed or fvcore needs to be installed for profiling to work.""
        self.profiler = profiler
        self.model.eval()
"
huggingface|pytorch-image-models|benchmark.py|020	Function	"    def run(self):
        _logger.info(
            f'Running profiler on {self.model_name} w/ '
            f'input size {self.input_size} and batch size {self.batch_size}.')
"
huggingface|pytorch-image-models|benchmark.py|021	Function	"def _try_run(
        model_name,
        bench_fn,
        bench_kwargs,
        initial_batch_size,
        no_batch_size_retry=False
):
    batch_size = initial_batch_size
    results = dict()
    error_str = 'Unknown'
    while batch_size:
        try:
            torch.cuda.empty_cache()
            bench = bench_fn(model_name=model_name, batch_size=batch_size, **bench_kwargs)
            results = bench.run()
            return results
        except RuntimeError as e:
            error_str = str(e)
            _logger.error(f'""{error_str}"" while running benchmark.')
            if not check_batch_size_retry(error_str):
                _logger.error(f'Unrecoverable error encountered while benchmarking {model_name}, skipping.')
                break
            if no_batch_size_retry:
                break
        batch_size = decay_batch_step(batch_size)
        _logger.warning(f'Reducing batch size to {batch_size} for retry.')
    results['error'] = error_str
    return results
"
huggingface|pytorch-image-models|benchmark.py|022	Function	"def benchmark(args):
    if args.amp:
        _logger.warning(""Overriding precision to 'amp' since --amp flag set."")
        args.precision = 'amp' if args.amp_dtype == 'float16' else '_'.join(['amp', args.amp_dtype])
    _logger.info(f'Benchmarking in {args.precision} precision. '
                 f'{""NHWC"" if args.channels_last else ""NCHW""} layout. '
                 f'torchscript {""enabled"" if args.torchscript else ""disabled""}')
"
huggingface|pytorch-image-models|benchmark.py|023	Function	"def main():
    setup_default_logging()
    args = parser.parse_args()
    model_cfgs = []
    model_names = []
"
huggingface|pytorch-image-models|benchmark.py|024	Function	"def write_results(results_file, results, format='csv'):
    with open(results_file, mode='w') as cf:
        if format == 'json':
            json.dump(results, cf, indent=4)
        else:
            if not isinstance(results, (list, tuple)):
                results = [results]
            if not results:
                return
            dw = csv.DictWriter(cf, fieldnames=results[0].keys())
            dw.writeheader()
            for r in results:
                dw.writerow(r)
            cf.flush()
"
huggingface|pytorch-image-models|bulk_runner.py|000	Function	"def cmd_from_args(args) -> Tuple[Union[Callable, str], List[str]]:
    # If ``args`` not passed, defaults to ``sys.argv[:1]``
    with_python = not args.no_python
    cmd: Union[Callable, str]
    cmd_args = []
    if with_python:
        cmd = os.getenv(""PYTHON_EXEC"", sys.executable)
        cmd_args.append(""-u"")
        if args.module:
            cmd_args.append(""-m"")
        cmd_args.append(args.script)
    else:
        if args.module:
            raise ValueError(
                ""Don't use both the '--no_python' flag""
                "" and the '--module' flag at the same time.""
            )
        cmd = args.script
    cmd_args.extend(args.script_args)
"
huggingface|pytorch-image-models|bulk_runner.py|001	Function	"def main():
    args = parser.parse_args()
    cmd, cmd_args = cmd_from_args(args)
"
huggingface|pytorch-image-models|bulk_runner.py|002	Function	"def write_results(results_file, results):
    with open(results_file, mode='w') as cf:
        dw = csv.DictWriter(cf, fieldnames=results[0].keys())
        dw.writeheader()
        for r in results:
            dw.writerow(r)
        cf.flush()
"
huggingface|pytorch-image-models|clean_checkpoint.py|000	Function	"def main():
    args = parser.parse_args()
"
huggingface|pytorch-image-models|clean_checkpoint.py|001	Function	"def clean_checkpoint(
        checkpoint,
        output,
        use_ema=True,
        no_hash=False,
        clean_aux_bn=False,
        safe_serialization: bool=False,
):
    # Load an existing checkpoint to CPU, strip everything but the state_dict and re-save
    if checkpoint and os.path.isfile(checkpoint):
        print(""=> Loading checkpoint '{}'"".format(checkpoint))
        state_dict = load_state_dict(checkpoint, use_ema=use_ema)
        new_state_dict = {}
        for k, v in state_dict.items():
            if clean_aux_bn and 'aux_bn' in k:
                # If all aux_bn keys are removed, the SplitBN layers will end up as normal and
                # load with the unmodified model using BatchNorm2d.
                continue
            name = k[7:] if k.startswith('module.') else k
            new_state_dict[name] = v
        print(""=> Loaded state_dict from '{}'"".format(checkpoint))
"
huggingface|pytorch-image-models|inference.py|000	Class	"An example inference script that outputs top-k class ids for images in a folder into a csv.
"
huggingface|pytorch-image-models|inference.py|001	Class	"                    help='path to class to idx mapping file (default: """")')
"
huggingface|pytorch-image-models|inference.py|002	Class	"                    help='include the class index in results')
"
huggingface|pytorch-image-models|inference.py|003	Function	"def main():
    setup_default_logging()
    args = parser.parse_args()
    # might as well try to do something useful...
    args.pretrained = args.pretrained or not args.checkpoint
"
huggingface|pytorch-image-models|inference.py|004	Function	"def save_results(df, results_filename, results_format='csv', filename_col='filename'):
    results_filename += _FMT_EXT[results_format]
    if results_format == 'parquet':
        df.set_index(filename_col).to_parquet(results_filename)
    elif results_format == 'json':
        df.set_index(filename_col).to_json(results_filename, indent=4, orient='index')
    elif results_format == 'json-records':
        df.to_json(results_filename, lines=True, orient='records')
    elif results_format == 'json-split':
        df.to_json(results_filename, indent=4, orient='split', index=False)
    else:
        df.to_csv(results_filename, index=False)
"
huggingface|pytorch-image-models|train.py|000	Class	"                   help='path to class to idx mapping file (default: """")')
"
huggingface|pytorch-image-models|train.py|001	Function	"def _parse_args():
    # Do we have a config file to parse?
    args_config, remaining = config_parser.parse_known_args()
    if args_config.config:
        with open(args_config.config, 'r') as f:
            cfg = yaml.safe_load(f)
            parser.set_defaults(**cfg)
"
huggingface|pytorch-image-models|train.py|002	Function	"def main():
    utils.setup_default_logging()
    args, args_text = _parse_args()
"
huggingface|pytorch-image-models|train.py|003	Function	"def train_one_epoch(
        epoch,
        model,
        loader,
        optimizer,
        loss_fn,
        args,
        device=torch.device('cuda'),
        lr_scheduler=None,
        saver=None,
        output_dir=None,
        amp_autocast=suppress,
        loss_scaler=None,
        model_ema=None,
        mixup_fn=None
):
    if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:
        if args.prefetcher and loader.mixup_enabled:
            loader.mixup_enabled = False
        elif mixup_fn is not None:
            mixup_fn.mixup_enabled = False
"
huggingface|pytorch-image-models|train.py|004	Function	"def validate(
        model,
        loader,
        loss_fn,
        args,
        device=torch.device('cuda'),
        amp_autocast=suppress,
        log_suffix=''
):
    batch_time_m = utils.AverageMeter()
    losses_m = utils.AverageMeter()
    top1_m = utils.AverageMeter()
    top5_m = utils.AverageMeter()
"
huggingface|pytorch-image-models|validate.py|000	Class	"                    help='path to class to idx mapping file (default: """")')
"
huggingface|pytorch-image-models|validate.py|001	Function	"def validate(args):
    # might as well try to validate something
    args.pretrained = args.pretrained or not args.checkpoint
    args.prefetcher = not args.no_prefetcher
"
huggingface|pytorch-image-models|validate.py|002	Function	"def _try_run(args, initial_batch_size):
    batch_size = initial_batch_size
    results = OrderedDict()
    error_str = 'Unknown'
    while batch_size:
        args.batch_size = batch_size * args.num_gpu  # multiply by num-gpu for DataParallel case
        try:
            if torch.cuda.is_available() and 'cuda' in args.device:
                torch.cuda.empty_cache()
            results = validate(args)
            return results
        except RuntimeError as e:
            error_str = str(e)
            _logger.error(f'""{error_str}"" while running validation.')
            if not check_batch_size_retry(error_str):
                break
        batch_size = decay_batch_step(batch_size)
        _logger.warning(f'Reducing batch size to {batch_size} for retry.')
    results['error'] = error_str
    _logger.error(f'{args.model} failed to validate ({error_str}).')
    return results
"
huggingface|pytorch-image-models|validate.py|003	Function	"def main():
    setup_default_logging()
    args = parser.parse_args()
    model_cfgs = []
    model_names = []
    if os.path.isdir(args.checkpoint):
        # validate all checkpoints in a path with same model
        checkpoints = glob.glob(args.checkpoint + '/*.pth.tar')
        checkpoints += glob.glob(args.checkpoint + '/*.pth')
        model_names = list_models(args.model)
        model_cfgs = [(args.model, c) for c in sorted(checkpoints, key=natural_key)]
    else:
        if args.model == 'all':
            # validate all models in a list of names with pretrained checkpoints
            args.pretrained = True
            model_names = list_models('convnext*', pretrained=True, exclude_filters=['*_in21k', '*_in22k', '*in12k', '*_dino', '*fcmae'])
            model_cfgs = [(n, '') for n in model_names]
        elif not is_model(args.model):
            # model name doesn't exist, try as wildcard filter
            model_names = list_models(args.model, pretrained=True)
            model_cfgs = [(n, '') for n in model_names]
"
huggingface|pytorch-image-models|validate.py|004	Function	"def write_results(results_file, results, format='csv'):
    with open(results_file, mode='w') as cf:
        if format == 'json':
            json.dump(results, cf, indent=4)
        else:
            if not isinstance(results, (list, tuple)):
                results = [results]
            if not results:
                return
            dw = csv.DictWriter(cf, fieldnames=results[0].keys())
            dw.writeheader()
            for r in results:
                dw.writerow(r)
            cf.flush()
"
facebookresearch|detectron2|setup.py|000	Function	"def get_version():
    init_py_path = path.join(path.abspath(path.dirname(__file__)), ""detectron2"", ""__init__.py"")
    init_py = open(init_py_path, ""r"").readlines()
    version_line = [l.strip() for l in init_py if l.startswith(""__version__"")][0]
    version = version_line.split(""="")[-1].strip().strip(""'\"""")
"
facebookresearch|detectron2|setup.py|001	Function	"def get_extensions():
    this_dir = path.dirname(path.abspath(__file__))
    extensions_dir = path.join(this_dir, ""detectron2"", ""layers"", ""csrc"")
"
facebookresearch|detectron2|setup.py|002	Function	"def get_model_zoo_configs() -> List[str]:
    """"""
    Return a list of configs to include in package for model zoo. Copy over these configs inside
    detectron2/model_zoo.
    """"""
"
pypa|pipenv|get-pipenv.py|000	Function	"# def determine_pip_install_arguments():
#     implicit_pip = True
# +    implicit_setuptools = False
# -    implicit_setuptools = True
#     implicit_wheel = True
#
#     # Check if the user has requested us not to install setuptools
# @@ -87,8 +60,6 @@
#
#     # We only want to implicitly install setuptools and wheel if they don't
#     # already exist on the target platform.
# +    # No need for doing this, since pipenv already has setuptools as
# +    # a dependency in setup.py
#     if implicit_setuptools:
#         try:
#             import setuptools  # noqa
# @@ -109,8 +80,6 @@
#         args += [""setuptools""]
#     if implicit_wheel:
#         args += [""wheel""]
# +
# +    args += [""pipenv""]
#
#     return [""install"", ""--upgrade"", ""--force-reinstall""] + args
"
pypa|pipenv|get-pipenv.py|001	Function	"def determine_pip_install_arguments():
    implicit_pip = True
    implicit_setuptools = False
    implicit_wheel = True
"
pypa|pipenv|get-pipenv.py|002	Function	"def monkeypatch_for_cert(tmpdir):
    """"""Patches `pip install` to provide default certificate with the lowest priority.
"
pypa|pipenv|get-pipenv.py|003	Function	"    def cert_parse_args(self, args):
        if not self.parser.get_default_values().cert:
            # There are no user provided cert -- force use of bundled cert
            self.parser.defaults[""cert""] = cert_path  # calculated above
        return install_parse_args(self, args)
"
pypa|pipenv|get-pipenv.py|004	Function	"def bootstrap(tmpdir):
    monkeypatch_for_cert(tmpdir)
"
pypa|pipenv|get-pipenv.py|005	Function	"def main():
    tmpdir = None
    try:
        # Create a temporary working directory
        tmpdir = tempfile.mkdtemp()
"
google|jax|conftest.py|000	Function	"def add_imports(doctest_namespace):
  import jax
  import numpy
  doctest_namespace[""jax""] = jax
  doctest_namespace[""lax""] = jax.lax
  doctest_namespace[""jnp""] = jax.numpy
  doctest_namespace[""np""] = numpy
"
google|jax|conftest.py|001	Function	"def pytest_collection() -> None:
  if not os.environ.get(""JAX_ENABLE_TPU_XDIST"", None):
    return
  # When running as an xdist worker, will be something like ""gw0""
  xdist_worker_name = os.environ.get(""PYTEST_XDIST_WORKER"", """")
  if not xdist_worker_name.startswith(""gw""):
    return
  xdist_worker_number = int(xdist_worker_name[len(""gw""):])
  os.environ.setdefault(""TPU_VISIBLE_CHIPS"", str(xdist_worker_number))
  os.environ.setdefault(""ALLOW_MULTIPLE_LIBTPU_LOAD"", ""true"")
"
google|jax|setup.py|000	Function	"def generate_proto(source):
  if not protoc or not os.path.exists(source):
    return
  protoc_command = [protoc, '-I.', '--python_out=.', source]
  if subprocess.call(protoc_command) != 0:
    sys.exit(-1)
"
